{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Author:** Josiah Wallis \\\n",
        "Created for use in CS/STAT108: Data Science Ethics (UCR - Winter 2024)\n",
        "***"
      ],
      "metadata": {
        "id": "FMe2Fz7FzjwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Statistical Models\n",
        "A **statistical model** is a mathematical model that utilizes statistical assumptions to describe a relationship between random variables. For our purposes, a random variable is a feature or column in our datasets. They are assumed to be probabilistic in nature. \\\n",
        "\\\n",
        "These models are commonly used for **prediction** â€” given past data and their associated labels, can we predict labels for unseen data? **Regression** involves predicting a **quantitative or continuous label** like money or temperature. **Classification** involves predicting predicting a **qualitative, categorical, or discrete label** like color or animal. In this notebook, we'll look at **simple linear regression**."
      ],
      "metadata": {
        "id": "V8doT7XMbA7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Linear Regression\n",
        "**Linear regression** is a linear prediction model that fits a line to data by minimizing its predictive error on the data. In other words, it attempts to model a relationship between one or more independent variables, or features, and a **quantitative** dependent variable, or target feature, using a line that has the least predictive error out of all possible linear regression models. The model can predict a label for said datapoint."
      ],
      "metadata": {
        "id": "yyYF_8sIc-pJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Linear Regression\n",
        "Assume a dataset has a trend that can be modeled by a line. **Simple linear regression** is a statistical model of the form\n",
        "$$y = \\theta_1x + \\theta_0 + \\varepsilon$$\n",
        "where\n",
        "\n",
        "\n",
        "*   $y$ = target feature\n",
        "*   $\\theta_i$ = parameters that define the model\n",
        "*   $\\varepsilon$ = random error\n",
        "\n",
        "$\\theta_0$ is often called the intercept or bias of the model. $\\varepsilon$ is the unexplainable error encountered in real-world problems, e.g. human error in recording the data, error in data recording software/hardware, etc. \\\n",
        "\n",
        "The goal in simple linear regression is to find an estimate $\\hat{y}$ of the true data trend $y$ that approximates the relationship between between our independent and dependent variables. More specifically, we want to find\n",
        "$$\\hat{y} = \\hat{\\theta_1}x + \\hat{\\theta_0}$$\n",
        "where $\\hat{\\theta_i}$ are our estimated model parameters. Though we won't delve into the entire process of finding these parameters, we'll see how to do it in practice below. \\\n",
        "\\\n",
        "I strongly encourage you have a browser pulled up while working through this notebook if you are unfamiliar with some of the concepts! I'll very often search \"plt.scatter\" or \"np.where\" to make sure I understand the function and what arguments they take. If you're curious what something is or how it truly works, google!\n"
      ],
      "metadata": {
        "id": "pfGW2JQC7vyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-qH9fgFc6ZA"
      },
      "outputs": [],
      "source": [
        "# Import libraries and dataset loader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "'''\n",
        "Load the dataframe.\n",
        "sklearn, a standard machine learning library, has many dataset loading functions for example datasets.\n",
        "'''\n",
        "bunch = fetch_california_housing(as_frame = True)\n",
        "df = bunch['frame']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `sklearn` `bunch` object is a dictionary-type object that contains all information relevant to the dataset it contains. Every `bunch` has a \"DESCR\" keyword that tells us about our dataset."
      ],
      "metadata": {
        "id": "IevHL32UEiBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bunch['DESCR'])"
      ],
      "metadata": {
        "id": "BkPr2xbPFXpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "67iuzy2BdhFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Let's take a look at our dataset! Output the first 10 samples in the dataset."
      ],
      "metadata": {
        "id": "XgFXih6HD0ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "vu2LxFVYD0An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) The **correlation coefficient** $\\rho$ is the measure of the linear relationship between two features $x$ and $y$ that ranges from -1 to 1 inclusively. The closer its absolute value is to 1, the stronger the linear relationship is between two features is. The sign of the coefficient determines the direction of their relationship. \\\n",
        "\n",
        "\n",
        "*   If $\\rho$ is **positive**, there is a **positive** linear trend between $x$ and $y$. An **increase** in $x$ *may* suggest an **increase** in $y$ linearly.\n",
        "*   If $\\rho$ is **negative**, there is a **negative** linear trend between $x$ and $y$. An **increase** in $x$ *may* suggest a **decrease** in $y$ linearly.\n",
        "*   A change in $x$ does *not* imply a change in $y$. Correlation suggests there may be a linear relationship between two variables, not a causal relationship.\n",
        "\n",
        "Check the correlation between each feature in our dataset using the `corr` method from `pandas`. What do these values tell you about the relationship between our features and our target feature: median house value? Do any features stand out?\n",
        "\n"
      ],
      "metadata": {
        "id": "zyjVylkSGeNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "7L2KrmXPMay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Grab the first 1000 points of the column with the highest correlation with median house value and store them in `X`. Store the first 1000 points of median house value in `y`. This pair will be the data we fit our simple linear regression model to."
      ],
      "metadata": {
        "id": "dekMYO2EMyUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "9pqCLZhaNrtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Visualization time! Use `plt.scatter` to plot your data on a scatter plot. Add some labels and other configurations that to make this plot look nice. Do you think there's a linear relationship between MedInc and MedHouseVal? \\\n",
        "*Hint*: the keyword arguments *alpha* and *s* may be useful for point visibility."
      ],
      "metadata": {
        "id": "Am8u5bMdN81L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "yTqeefGMNr5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) **(OPTIONAL)** Experiment with different visualization graphs and plots! Feel free to import `seaborn` as well."
      ],
      "metadata": {
        "id": "P3h8YIeNQ-4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "6PRt8QybNsTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting our model"
      ],
      "metadata": {
        "id": "cApqAbYRdnDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's format our data before fitting a model to it. First, run this cell to turn our data into numpy arrays with the proper shape that our future linear regression class will accept."
      ],
      "metadata": {
        "id": "usiwPVyLVOSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.to_numpy().reshape(len(X), 1)\n",
        "y = y.to_numpy()"
      ],
      "metadata": {
        "id": "fhYf7j_7NskJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Because we don't have \"unseen\" data, we can simulate unseen data by only having our model fit on a portion of our data while testing its predictive power on the rest of the data. This is called a **train-test split** where our **training data** is the data the model fits while the **test data** is the data we use to evaluate our model's performance on unseen data. We'll use `sklearn.model_selection.train_test_split` to do this for us ([link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Import the method and make a train-test split with a test size of 20%. Verify that the training set and test set are the correct shapes."
      ],
      "metadata": {
        "id": "ee8M0eUeTaRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n",
        "\n",
        "# Import necessary function\n",
        "\n",
        "X_train, X_test, y_train, y_test = #fill"
      ],
      "metadata": {
        "id": "uWLvtO-TUQHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import a linear regression model from a machine learning library: `sklearn`! Check out the object we're working with [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). It will produce a line that best models the relationship between MedInc and MedHouseVal. This model will have the least sum of squared errors between these two variables. \\\n",
        "\\\n",
        "In the cell below, we'll import then fit our model to our training data."
      ],
      "metadata": {
        "id": "zKGHzYUuS4jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PP_JUZuFcWe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! The `fit` method did all the heavy lifting for us without us having to do any computations or the complicated code behind linear regression. If you're interested, a brief intro can be found [here](https://brilliant.org/wiki/linear-regression/)! Let's check out what estimates this model found for our $\\hat{\\theta_1}$ and $\\hat{\\theta_0}$."
      ],
      "metadata": {
        "id": "sPSdLZulXC1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_1_hat = model.coef_[0]\n",
        "theta_0_hat = model.intercept_\n",
        "print(f'Slope: {theta_1_hat}\\nbias/intercept: {theta_0_hat}')"
      ],
      "metadata": {
        "id": "mpQh48P6dLOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot our line and data on the same figure to see how well the line fits our data."
      ],
      "metadata": {
        "id": "LQa1igDkZMSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval = np.linspace(np.min(X_train), np.max(X_train), 20)\n",
        "\n",
        "plt.scatter(X_train, y_train, s = 5)\n",
        "plt.plot(interval, theta_1_hat * interval + theta_0_hat, color = 'lime')\n",
        "plt.title('SLR fit to MedInc vc MedHouseVal')\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LMPyReoydOEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems like a pretty good fit! Though there may be other nonlinear or clustering models that can describe the data better, this is the best linear regression fit to our data when using squared error as our metric.\\\n",
        "\\\n",
        "Our estimated model is\n",
        "$$\\hat{y} = 0.38x + 0.63.$$\n",
        "This can be interpreted as \"*for a unit increase in median income, median house value increases by $38,000 on average*.\""
      ],
      "metadata": {
        "id": "Y01xGfPka9Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Before we use this model to predict values for our test data, let's plot this line on our test data. Does the line fit the test data well? \\\n",
        "*Hint*: `interval` is still usable from the last cell."
      ],
      "metadata": {
        "id": "zuvMGCtpbPWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n"
      ],
      "metadata": {
        "id": "rccxs4kIYmFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction time! Let's predict on our test data. In this simulation, we luckily we know the true label for each data point in our test data. Therefore, we have an idea of how good the predictions are."
      ],
      "metadata": {
        "id": "9qY4shIucO1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model.predict(X_test)\n",
        "n = 5\n",
        "\n",
        "# Comparing our predicted vs actual values\n",
        "print(f'Predicted values: {y_hat[:n]}\\nActual values: {y_test[:n]}')"
      ],
      "metadata": {
        "id": "Rx88VYIOb_03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Awesome! We've done our first predictions. Do these seem like good estimates? What factors could explain the discrepancy in our estimates versus the true values?"
      ],
      "metadata": {
        "id": "oTHl5_lbdVRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FxTHOM8QfCfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) There are many performance metrics for measuring how well a model is predicting. One of these metrics is **mean-squared error**, the average of the sum of squared errors. It is a common metric used in regression problems and is computed as\n",
        "\n",
        "$$MSE = \\frac1n \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2.$$\n",
        "\n",
        "\n",
        "*   $n$ i the number of data points being used\n",
        "*   $y_i$ is the true label of $x_i$\n",
        "*   $\\hat{y}_i$ is the predicted label of $x_i$\n",
        "\n",
        "You can use the $MSE$ of two models to get an idea of how good one model is over the other for a specific prediction case. Typically, a lower MSE signals that model may have a greater predictive power than another. For more details, click [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/).\\\n",
        "\\\n",
        "Write a function `mse` that takes as arguments the true and predicted labels of a dataset and computes their mean-squared error. Then, compute the Test MSE - the MSE between the test data labels and their predicted labels. Compare to `sklearn.metrics.mean_squared_error`."
      ],
      "metadata": {
        "id": "z0WlWQK4fCzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code below\n",
        "\n",
        "# Import necessary function for comparison\n"
      ],
      "metadata": {
        "id": "uWKruConcvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "**Congratz on finishing the notebook!** You now know how to\n",
        "\n",
        "\n",
        "*   load data from `sklearn`\n",
        "*   plot a scatter plot with `plt`\n",
        "*   perform a train-test split using `sklearn`\n",
        "*   fit a simple linear regression model using `sklean`\n",
        "*   find the parameters of your model\n",
        "*   plot your regression line\n",
        "*   interpret model parameters\n",
        "*   predict using simple linear regression\n",
        "\n",
        "Every model has different pros and cons: some predict better on more complicated data, some generalize better, some we can hardly interpret! Additionally, there are tools for judging performance that are specific to regression models that do not work well with models trained for classification (like MSE), and vice versa. At the end of the day, all predictive models used for regression take in data to learn from, then they can be used to predict continuous labels for unseen data. \\\n",
        "\\\n",
        "Next week, we'll talk about **logistic regression.** Though it's called \"regression,\" it is used for the classification task. It learns from data, then it is used to predict categorical or discrete labels from 2 class. An example is learning from images of numbers, then determining if an image is a 5 or not."
      ],
      "metadata": {
        "id": "ZDC3h16HpHHt"
      }
    }
  ]
}