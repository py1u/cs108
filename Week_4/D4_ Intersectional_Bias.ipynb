{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8EW05Ejelt"
      },
      "source": [
        "Intersectionality in Mental Health Care\n",
        "=======================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh0YkYBejelu"
      },
      "source": [
        "*This notebook was written by Yifan Wang, Marta Maslej, and Laura\n",
        "Sikstrom and is licenced under a* [Creative Commons Attribution 4.0\n",
        "International License](http://creativecommons.org/licenses/by/4.0/).\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "There is an increasing interest in applying innovations in artificial\n",
        "intelligence to provide more efficient, precise, or personalized patient\n",
        "care. Specifically, the ability of machine learning (ML) to identify\n",
        "patterns in large and complex datasets holds tremendous promise for\n",
        "solving some of the most complex and intractable problems in health\n",
        "care. However, there are ongoing questions\n",
        "`ghassemi2022medicine`{.interpreted-text role=\"footcite\"} about a range\n",
        "of known **gendered and racialized biases** - arising from diagnostic\n",
        "tools, clinical interactions, and health policies -that get *baked* into\n",
        "these datasets. In nephrology, for example, algorithms developed to\n",
        "estimate glomerular filtration rate assign higher values (which suggest\n",
        "better kidney function) to Black individuals\n",
        "`vyas2020hidden`{.interpreted-text role=\"footcite\"}, which could delay\n",
        "treatent for this patient group while ultimately worsening outcomes. In\n",
        "turn, any ML algorithms that are trained on this data could exhibit\n",
        "**differences in predictive performance across certain groups** - not by\n",
        "any flaw of the algorithm itself, but because it is capturing societal\n",
        "biases that are encoded into the data itself. Upon deployment, these\n",
        "tools can *amplify harms* for marginalized populations, particularly\n",
        "those defined by intersections of features (e.g., gender, race, class).\n",
        "\n",
        "In this tutorial, we will examine another instance of this: in\n",
        "psychiatric diagnosis data, Black men are much more likely to be\n",
        "misdiagnosed with schizophrenia as compared to white men due to factors\n",
        "such as diagnostic bias by clinicians. Through this case study, we\n",
        "demonstrate the **value of applying an interdisciplinary approach to\n",
        "analyzing intersectional biases**, towards ensuring that these\n",
        "innovative tools are implemented safely and ethically.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Learning objectives**. This notebook has three main learning\n",
        "objectives. After this tutorial, we hope that you will be able to:\n",
        "\n",
        "1.  Think critically about how populations can be defined and how this\n",
        "    relates to the measurement, identification, and interpretation of\n",
        "    health inequities\n",
        "2.  Explain the advantages of an intersectional approach to research\n",
        "3.  Conduct an intersectional bias assessment of simulated psychiatric\n",
        "    data, based on a finding from the psychiatric literature\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "What is a fair machine learning model?\n",
        "======================================\n",
        "\n",
        "Sikstrom et al (2022) identify 3 general pillars of fairness as it\n",
        "pertains to ML: Transparency, Inclusion, and Impartiality\n",
        "`sikstrom2022conceptualising`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "**Transparency**: A range of methods designed to see, understand, and\n",
        "hold complex algorithmic systems accountable in a timely fashion.\n",
        "\n",
        "**Inclusion**: The process of improving the ability, opportunity, and\n",
        "dignity of people, disadvantaged on the basis of their identity, to\n",
        "access health services, receive compassionate care, and achieve\n",
        "equitable treatment outcomes.\n",
        "\n",
        "**Impartiality**: Health care should be free from unfair bias and\n",
        "systemic discrimination. Deploying a ML algorithm requires a\n",
        "sociotechnical understanding of how data is collected and interpreted\n",
        "within algorithmic system in order to ensure its responsible\n",
        "implementation in a clinical setting.\n",
        "\n",
        "Although all aspects of fairness are equally important, this tutorial\n",
        "will focus on impartiality. Specifically, we aim to examine fairness\n",
        "through the scope of intersectionality, which was originally coined by\n",
        "Kimberle Crenshaw `crenshaw1991intersectionality`{.interpreted-text\n",
        "role=\"footcite\"}:\n",
        "\n",
        "> **Intersectionality** is a framework for understanding how different\n",
        "> forms of inequality (e.g., gender and race) often operate together and\n",
        "> exacerbate each other.\n",
        "\n",
        "While by no means exhaustive, this serves as a useful frame that can be\n",
        "coupled with other fairness approaches to enable a more thoughtful\n",
        "discussion of fairness-related issues.\n",
        "\n",
        "Who and what is a population?\n",
        "=============================\n",
        "\n",
        "When we do a fairness assessment, we need to decide which groups of\n",
        "people to compare to identify whether there is some kind of\n",
        "fairness-related harm occurring.\n",
        "\n",
        "The key question in being able to do so is this: **Who and what is a\n",
        "population?** It may seem like this question is trivial, and has a\n",
        "clear-cut meaning in no need of further clarification. However,\n",
        "researchers like Nancy Krieger have pointed out that a clear notion of\n",
        "\\\"population\\\" is rarely defined\n",
        "`krieger2012population`{.interpreted-text role=\"footcite\"}, despite its\n",
        "centrality to fields like ML fairness.\n",
        "\n",
        "As such, seeking a clearer answer to this question is central to ML\n",
        "because it enables us to determine how and when can populations be\n",
        "**meaningfully and appropriately compared**, and allows a recognition of\n",
        "when such comparisons may be meaningless, or even worse, misleading.\n",
        "\n",
        "A reflective activity\n",
        "---------------------\n",
        "\n",
        "*\\\"Every man is in certain respects, like all other men, like some other\n",
        "men, like no other men.\\\"* \\-- Murray and Klukholne (1953).\n",
        "\n",
        "> Consider a group or community that you\\'ve been part of. This could be\n",
        "> anything from group of friends or colleagues, to the people you\\'re\n",
        "> currently sitting next to. Consider the following questions:\n",
        ">\n",
        "> -   *What is something that your whole group has in common?* For\n",
        ">     example, on a soccer team, this might be the fact that everyone\n",
        ">     plays soccer.\n",
        "> -   *What is something that some of your group has in common?* Going\n",
        ">     off the same soccer team example, perhaps some of the team\n",
        ">     identifies as boys, while some of the team identifies as girls.\n",
        "> -   *What is something that makes each of you unique?* Perhaps\n",
        ">     everyone is a different height, or everyone grew up in a different\n",
        ">     neighborhood.\n",
        "\n",
        "First, notice the intersectional approach that we took with this\n",
        "activity - each individual is not defined by features about themselves\n",
        "in isolation, but the intersection of many different identities, which\n",
        "constitutes the experience of that individual.\n",
        "\n",
        "Second, note that disparities can result from both the factors that make\n",
        "us the same, and the factors that make us different. We need to keep\n",
        "this idea in mind - when we\\'re comparing groups, are these meaningful\n",
        "differences that we\\'re comparing, and how do we know? For example, a\n",
        "potential similarity between a group of people is that \\\"all of us wear\n",
        "glasses\\\" - but **does this constitute a meaningful way to compare\n",
        "groups of people?** Answering this question requires context, and can\n",
        "help us identify biases in our judgement that can lead to fairness\n",
        "harms, and think about possible solutions.\n",
        "\n",
        "Let\\'s look at a real-life example to see how exactly this is important.\n",
        "\n",
        "In 2011, a paper studying the epidemiology of cancer noted that \\\"early\n",
        "onset ER negative tumors also develop more frequently in Asian Indian\n",
        "and Pakistani women and in women from other parts of Asia, although not\n",
        "as prevalent as it is in West Africa.\\\"\n",
        "`wallace2011interactions`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "At first glance, this seems like a reasonable comparison that helps\n",
        "establish the basis for certain populations having a higher prevalence\n",
        "of cancer.\n",
        "\n",
        "However, Krieger points out that the cancer incidence rates used to\n",
        "arrive at this conclusion are based on:\n",
        "\n",
        "-   For Pakistan, the weighted average of observed rates within a single\n",
        "    region\n",
        "-   For India, a complex estimation involving several rates in different\n",
        "    states\n",
        "-   For West Africa, the weighted average for 16 countries:\n",
        "    -   10 of these countries have rates estimated based on neighboring\n",
        "        countries\n",
        "    -   5 rely on extrapolation from a single city within that country\n",
        "    -   Only one has a national cancer registry\n",
        "\n",
        "This added context makes it clear that these population comparisons are\n",
        "not so clear-cut, and that perhaps there is more nuance we need to be\n",
        "mindful of than we first thought.\n",
        "\n",
        "Defining a population\n",
        "---------------------\n",
        "\n",
        "How then, should we conceptualize populations to enable the nuanced\n",
        "understanding required when we compare them? To give some background on\n",
        "this work, we will again draw on some of the work of Nancy Krieger, an\n",
        "epidemiologist who has written extensively on the concept of\n",
        "populations.\n",
        "\n",
        "### Populations as statistical entities\n",
        "\n",
        "Much of Krieger\\'s work stands in contrast to the conventional\n",
        "definition of population, which is limited to an understanding of\n",
        "populations as statistical objects, rather than of substantive beings.\n",
        "This definition is as follows:\n",
        "\n",
        "> **Statistical view on populations.** Populations are (statistical)\n",
        "> entities composed of component parts defined by **innate (or\n",
        "> intrinsic)** attributes\\'\n",
        "\n",
        "Implicit in this definition is a notion of causality: if populations\n",
        "differed in their means, this indicated that there was either a\n",
        "difference in \\\"essence, or in or external factors, that caused this\n",
        "difference between populations. To this end, **populations can be\n",
        "compared on the basis of their means because they are caused by\n",
        "comparable differences.**\n",
        "\n",
        "This idea that humans have **innate and comparable** attributes was\n",
        "largely derived from Adolphe Quetelet\\'s invention of the **\\\"Average\n",
        "Man,\\\"** `eknoyan2007adolphe`{.interpreted-text role=\"footcite\"}\n",
        "establishing the notion of a population mean.\n",
        "\n",
        "Originally a physicist, Quetelet borrowed the idea from astronomy, where\n",
        "the \\\"true location\\\" of a single star was determined the observations\n",
        "done by multiple observatories. Applied to populations, this meant that\n",
        "observing the characteristics of multiple/all the individuals within a\n",
        "group allowed the establishment of a \\\"true mean\\\" like human height, or\n",
        "body weight (which is how we got the Body Mass Index).\n",
        "\n",
        "While the population mean of a star is a descriptor of its position in\n",
        "space, the population mean of a human population depends on how that\n",
        "population is defined. For example, recall our reflective activity:\n",
        "those similarities and differences constitute how one might define a\n",
        "group. This raises some interesting questions and issues.\n",
        "\n",
        "### The issues\n",
        "\n",
        "> **\\\"For a star, the location of the mean referred to the location of a\n",
        "> singular real object, whereas for a population, the location of a\n",
        "> population mean**depended on how the population was defined.\\\"\\*\\*\n",
        "\n",
        "We could define a population as all those who are human, or perhaps all\n",
        "those who are of a certain nationality, or many other possibilities.\n",
        "Thinking back about the reflective activity, the elements that your\n",
        "group had in common and those that were different are all elements that\n",
        "could help define a population.\n",
        "\n",
        "Crucially, we need to be careful about how we define a population,\n",
        "because it can impact the results we get from any analyses - perhaps it\n",
        "might skew them, or perhaps the lack of appropriate comparison groups\n",
        "might render an analysis inappropriate. Let\\'s look an example of this\n",
        "from a 2016 BBC article `bbc2016dutch`{.interpreted-text\n",
        "role=\"footcite\"} which compares body heights:\n",
        "\n",
        "![A visualization that shows the average height of different countries in 1914 and 2014.\n",
        "The Netherlands has the tallest average body height for men, Iran the biggest height gain\n",
        "for men, South Korea the biggest height gain for women, and Guatemala the smallest average\n",
        "body height for women.](../_static/images/imhc_bodyheight.png){width=\"600px\"}\n",
        "\n",
        "On the basis of what population definitions are these comparisons being\n",
        "made? In this case, comparisons are being made between genders,\n",
        "nationalities, and time frames.\n",
        "\n",
        "But consider this - what exactly makes it meaningful to compare 1914 to\n",
        "2014? How can we truly interpret this data? We don\\'t have a frame of\n",
        "reference for how each population is defined. We don\\'t know their ages,\n",
        "we don\\'t how the data was collected, and we don\\'t know why nationality\n",
        "was considered an appropriate characteristic for comparison. These\n",
        "elements are **critical in establishing the context for whether\n",
        "something can be considered a meaningful comparison**, and in their\n",
        "absence, we are left questioning.\n",
        "\n",
        "This article is hardly unique in this problem - body mass index (BMI)\n",
        "has become a ubiquitous way of defining obesity, but comparing\n",
        "individuals on the basis of their BMI is problematic: the scale was\n",
        "built by and for white populations, leading to an overestimation of\n",
        "health risks for Black individuals\n",
        "`endocrine2009widely`{.interpreted-text role=\"footcite\"}, and an\n",
        "underestimation of health risks for Asian individuals\n",
        "`racette2003obesity`{.interpreted-text role=\"footcite\"}. Even more\n",
        "interestingly, BMI was never meant to be a way of measuring health at an\n",
        "individual level. `karasu2016adolphe`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "The point is this: **social relations, not just individual traits, shape\n",
        "population distributions of health.**\n",
        "\n",
        "What this means is that we cannot exclude the context that social\n",
        "relations *between* populations illuminate. This could include anything\n",
        "from gender dynamics, to class disparities, to differing economic\n",
        "systems - all of which can have differing impacts on human health.\n",
        "\n",
        "This point is especially important because of how **heterogeneous humans\n",
        "are**, across space and time, both between groups and within groups.\n",
        "Someone who has written extensively about this in relation to ML,\n",
        "disability, and design is Jutta Treviranus.\n",
        "\n",
        "Treviranus points out that the further you get from a population mean,\n",
        "the more heterogeneous the population gets. And as you move away from\n",
        "this \\\"average mean\\\" - not only do people get more diverse from one\n",
        "another, but solutions that are designed with the population mean in\n",
        "mind are increasingly ineffective.\n",
        "`treviranus2019inclusivedesign`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "![A visualization that shows scattered dots.\n",
        "The further you move away from the middle, the more scattered the points are.\n",
        "Around the points in the center, a blue circle is drawn that points to \\'design works\\'.\n",
        "Moving further from the center, a second yellow circle points to \\'design is difficult\n",
        "to use\\'. Moving even further from the center, a third orange circle points to \\'can\\'t\n",
        "use design\\'.](../_static/images/imhc_design.png){width=\"400px\"}\n",
        "\n",
        "Notice the parallel we are drawing throughout here - we must always be\n",
        "thinking about how we compare people, and how we can make those\n",
        "comparisons meaningful.\n",
        "\n",
        "### Proposition 2\n",
        "\n",
        "Given these limitations, let\\'s consider an alternative understanding of\n",
        "a population that :footcite`krieger2012population`{.interpreted-text\n",
        "role=\"ct\"} proposes.\n",
        "\n",
        "> Populations are dynamic beings constituted by intrinsic relationships\n",
        "> both among their members and with other populations that together\n",
        "> produce their existence and make causal inference possible.\n",
        "\n",
        "What Krieger points out here, is that to make conclusions about\n",
        "differences between populations, we need to understand **how these\n",
        "populations came to be, and how we define them.**\n",
        "\n",
        "Krieger\\'s view emphasizes that **identity is not a fixed attribute\n",
        "(like a star), but rather a fluid social process**. A great example that\n",
        "illustrates the degree to which identity can change can be seen in the\n",
        "racial identity of Puerto Ricans in the aftermath of Hurricane Maria.\n",
        "`godreau2021nonsovereign`{.interpreted-text role=\"footcite\"} Following\n",
        "the hurricane, there was a 72% drop in the amount of Puerto Ricans\n",
        "identifying as white. Did all the white people move away? Nope - they\n",
        "just no longer thought of themselves as white. The significant shift can\n",
        "be largely attributed to Puerto Ricans feeling as though they had been\n",
        "neglected by the US government in their inadequate response to the\n",
        "hurricane, and as such, redefined themselves as Latinx. That\\'s an\n",
        "entire population collectively deciding to change their racial identity,\n",
        "and gives an example of just how dynamic - and unpredictable - identity\n",
        "can be.\n",
        "\n",
        "This definition of populations also recognizes that this social process\n",
        "is **linked to a history of racism and sexism**. For instance, people\\'s\n",
        "skin color was taken as being intrinsically related to specific\n",
        "characteristics - a stereotype which has promoted all sorts of\n",
        "inequalities in today\\'s society.\n",
        "\n",
        "A notion of populations that emphasizes their dynamic nature is helpful\n",
        "because it helps understand the relationships between these different\n",
        "groups, like gender, class, and religion -all social constructions that\n",
        "have meaning for us. It acknowledges that **we\\'re all working within\n",
        "these systems through various different lenses, and within different\n",
        "dynamics of power and privilege**. The graphic below is a great\n",
        "representation of that - of how varied all of our different experiences\n",
        "can be:\n",
        "\n",
        "> *Intersectionality is a lens through which you can see where power\n",
        "> comes and collides, where it locks and intersects. It is the\n",
        "> acknowledgement that everyone has their own unique experiences of\n",
        "> discrimination and privilege.* \\--Kimberle Crenshaw\n",
        "\n",
        "![In the graphic above, Sylvia Duckworth uses a Spirograph to illustrate the multitude of\n",
        "ways that social identities might intersect. The Spirograph is split into 12 overlapping\n",
        "circles, each numbered, connected to a specific social identity, and assigned a unique\n",
        "colour. To illustrate the intersections of the different social identities, where each\n",
        "circle intersects, a new shade of the original colour is visible (as would happen when\n",
        "mixing paint colours together). At a glance the graphic shows all colours of the rainbow\n",
        "in different shades.\n",
        "The 12 social identities listed are: race, ethnicity, gender identity,\n",
        "class, language, religion, ability, sexuality, mental health, age, education,\n",
        "and body size.\n",
        "A quote from Kimberlé Crenshaw appears beneath the spirograph that reads\n",
        "\"Intersectionality\n",
        "is a lens through which you can see where power comes and collides, where it locks and\n",
        "intersects.\n",
        "It is the acknowledgement that everyone has their own unique experiences of\n",
        "discrimination and privilege.\" \\\"Intersectionality\\\" by Sylvia Duckworth, licensed under a\n",
        "\\`CC-BY-NC-ND\\`\\<https://creativecommons.org/licenses/by-nc-nd/2.0/\\> license.](../_static/images/imhc_intersectionality.jpg){width=\"600px\"}\n",
        "\n",
        "Critical race theory\n",
        "--------------------\n",
        "\n",
        "### Ain\\'t I A Woman?\n",
        "\n",
        "Note that although this intersectional approach has only recently been\n",
        "applied to ML, it is not a novel concept. Rather, it's a\n",
        "well-established idea that stretches back nearly 200 years to the Black\n",
        "feminist and the suffragette movement, when white women were fighting\n",
        "for the right to vote, but Black women were left out of this call for\n",
        "reform. There were a range of Black activists and scholars who responded\n",
        "to this - wondering \\\"What about me? Don\\'t I matter?\\\"\n",
        "\n",
        "For example, let us consider a quote from Sojourner Truth - an enslaved\n",
        "person at the time. `npsSojournerTruth`{.interpreted-text\n",
        "role=\"footcite\"}\n",
        "\n",
        "> That man over there says that women need to be helped into carriages,\n",
        "> and lifted over ditches, and to have the best place everywhere. Nobody\n",
        "> ever helps me into carriages, or over mud-puddles, or gives me any\n",
        "> best place! And ain\\'t I a woman? Look at me! Look at my arm! I have\n",
        "> ploughed and planted, and gathered into barns, and no man could head\n",
        "> me! And ain\\'t I a woman? I could work as much and eat as much as a\n",
        "> man - when I could get it - and bear the lash as well! And ain\\'t I a\n",
        "> woman? I have borne thirteen children, and seen most all sold off to\n",
        "> slavery, and when I cried out with my mother\\'s grief, none but Jesus\n",
        "> heard me! And ain\\'t I a woman?\n",
        "\n",
        "What she\\'s saying is \\\"I am a woman - yet all these social factors,\n",
        "like slavery, have somehow disenfranchised me from this feminist\n",
        "movement that\\'s going on.\\\" In asking \\\"Aint I A Woman?\\\", Sojourner\n",
        "Truth is really asking the question that\\'s been central to our\n",
        "discussion - what makes a population? Defining a population a specific\n",
        "way, like ignoring the realities of Black women, can cause a lot of\n",
        "harm.\n",
        "\n",
        "### Critical race theory\n",
        "\n",
        "> **Critical race theory** is an iterative methodology that draws on the\n",
        "> collective wisdom of activists and scholars to study and transform the\n",
        "> relationship between race, racism, and power. \\--\n",
        "> :footcite`ford2010critical`{.interpreted-text role=\"ct\"}\n",
        "\n",
        "Intersectionality is one tool within the critical race theory toolkit,\n",
        "among many others, and there has already been some great work\n",
        "`hanna2020towards`{.interpreted-text role=\"footcite\"} done on how\n",
        "critical race theory (CRT) may be applied to algorithmic fairness, such\n",
        "as the need for disaggregated analysis that operates on a descriptive\n",
        "level in order to interrogate the most salient aspects of race for an\n",
        "algorithmic system. A central element of CRT, especially as it relates\n",
        "to ML systems, echoes a theme we have already discussed extensively:\n",
        "**how we define a group of people matters.** The power to define a group\n",
        "of people matters, and we need to question our assumptions about what\n",
        "makes people have something in common, and how that might affect our\n",
        "ability to compare populations.\n",
        "\n",
        "The practical applications to machine learning\n",
        "----------------------------------------------\n",
        "\n",
        "In practice, the focus on these intersectional comparisons within the ML\n",
        "field has been on protected groups\n",
        "`justiceCanadianHuman`{.interpreted-text role=\"footcite\"}, such as:\n",
        "\n",
        "-   Race, national or ethnic origin, color\n",
        "-   Religion\n",
        "-   Age\n",
        "-   Sex\n",
        "-   Sexual orientation\n",
        "-   Gender identity or expression\n",
        "-   Family status\n",
        "-   Marital status\n",
        "-   Disability, genetic characteristics\n",
        "\n",
        "We have legal obligations to ensure that any medical interventions do\n",
        "not discriminate against individuals on the basis of these attributes,\n",
        "so it makes sense that most of the ML fairness assessment have taken\n",
        "place on race, sex or gender.\n",
        "\n",
        "However, it\\'s important to note that protected groups vary\n",
        "substantially between countries, and even across specific applications\n",
        "within a country. Within the United States, for example, the Fair\n",
        "Housing Act recognizes disability and gender identity as protected\n",
        "classes while the Equal Credit Opportunity Act does not.\n",
        "\n",
        "Furthermore, the social determinants of health that are known to impact\n",
        "health inequalities, such as class, language, citizenship status (e.g.,\n",
        "undocumented), and geography (e.g., rural vs urban) are not protected\n",
        "groups. How these issues are understood and measured in the literature\n",
        "are quite varied. This means that we understand *some* ways that ML can\n",
        "lead to fairness harms, but there are **many fairness harms happening\n",
        "that we know almost nothing about.**\n",
        "\n",
        "Case Study: Mental Health Care\n",
        "==============================\n",
        "\n",
        "We now turn to a case study on a hypothetical scenario, where we train a\n",
        "machine learning model to predict a patient\\'s diagnosis in a mental\n",
        "health care setting.\n",
        "\n",
        "::: {.note}\n",
        "::: {.title}\n",
        "Note\n",
        ":::\n",
        "\n",
        "The scenario considered in this case study is **overly simplified** and\n",
        "based on a **simulated data set**. But we hope it gives you a sense of\n",
        "how the application of ML can exacerbate biases in training data, and\n",
        "how we can evaluate models for bias from an intersectional perspective.\n",
        ":::\n",
        "\n",
        "Scenario\n",
        "--------\n",
        "\n",
        "To highlight the way that fairness harms has been evaluated in health\n",
        "care, we will first examine a study by\n",
        ":footcite`obermeyer2019dissecting`{.interpreted-text role=\"ct\"}.\n",
        "\n",
        "### Algorithmic Fairness in Health Care\n",
        "\n",
        ":footcite`obermeyer2019dissecting`{.interpreted-text role=\"ct\"} examined\n",
        "a risk assessment algorithm for health insurance across approximately\n",
        "**50000 primary care patients**. This algorithm was used to identify\n",
        "individuals who required further follow-up for complex care.\n",
        "\n",
        "The authors found that this algorithm was **underestimating disease\n",
        "severity in Black clients.** Why might this be the case?\n",
        "\n",
        "They proposed several issues:\n",
        "\n",
        "First, the algorithm used **total health expenditures as a proxy for\n",
        "disease severity.** This is problematic because health expenditures\n",
        "might vary based on socioeconomic status, and poverty levels in Black\n",
        "populations tend to be higher. As such, even if someone is very sick,\n",
        "they may be unwilling or unable to spend money on health care. There may\n",
        "also be an issue of trust involved, but in short, the authors didn\\'t\n",
        "truly understand what was going on.\n",
        "\n",
        "One of the things they point out, however, is that the bias which they\n",
        "encountered - termed \\\"Labelling Bias\\\" - is pernicious, because labels\n",
        "are measured with inequities built into them. In other words, when the\n",
        "labels on a dataset carry structural inequities, those biases are\n",
        "unknowingly built in (we will examine this further in the applied\n",
        "component of this tutorial).\n",
        "\n",
        "The authors also note that although the developers of the algorithm\n",
        "assumed that Black and white people had some meaningful difference, they\n",
        "**didn\\'t distinguish between the two groups** based on income or gender\n",
        "- this likely led to an underestimation of how poorly this model might\n",
        "be estimating risk for certain groups of people.\n",
        "\n",
        "A similar issue is presented by Buolamwini and Gebru in the gender\n",
        "shades project, which analyzed fairness issues in facial detection\n",
        "software and found that the AI system led to larger errors for\n",
        "dark-skinned women than for other groups.\n",
        "`buolamwini2018gender`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "With the problems that Obermeyer and colleagues investigated related to\n",
        "using proxies as labels and keeping intersectionality in mind, let's\n",
        "move forward with an applied problem of our own.\n",
        "\n",
        "### Background: Schizophrenia\n",
        "\n",
        "Our case scenario for this practice component of the tutorial is based\n",
        "on the finding that **Black patients (and men in particular) are\n",
        "diagnosed with schizophrenia at a higher rate than other demographic\n",
        "groups** (for example, white men). `olbert2018meta`{.interpreted-text\n",
        "role=\"footcite\"}\n",
        "\n",
        "> **Schizophrenia is a severe, chronic, and debilitating illness**\n",
        "> characterized by various symptoms, which are broadly divided into\n",
        "> positive and negative symptoms. Positive symptoms include the symptom\n",
        "> types that generally come to mind when thinking about schizophrenia,\n",
        "> like delusions, hallucinations, disorganized speech and thought, but\n",
        "> negative symptoms are also common. These include a lack of emotion,\n",
        "> motivation, or interest.\n",
        "\n",
        "It's unclear why Black patients are more likely to be diagnosed with\n",
        "this disorder, but it's likely that different factors play a role, such\n",
        "as genetics or being more likely to reside in stressful environments\n",
        "(which can be a risk factor for developing schizophrenia). However,\n",
        "there is also some pretty compelling evidence\n",
        "`gara2019naturalistic`{.interpreted-text role=\"footcite\"} that this\n",
        "effect is **due to diagnostic bias**, or clinicians misdiagnosing black\n",
        "patients with schizophrenia when they have another illness, like an\n",
        "affective disorder or depression instead. Clinicians may be\n",
        "underemphasizing symptoms of depression in black patients and\n",
        "overemphasizing psychotic symptoms, leading to misdiagnosis (and higher\n",
        "rates of schizophrenia). This tendency may be **particularly pronounced\n",
        "for Black men.**\n",
        "\n",
        "Why is this important?\n",
        "\n",
        "**Misdiagnosis can have negative downstream effects**\n",
        "`gara2012influence`{.interpreted-text role=\"footcite\"}, leading to\n",
        "inequities in care. Schizophrenia is a more serious and stigmatized\n",
        "illness than affective disorder, it has a poorer prognosis, and involves\n",
        "treatments with greater side effects. Misdiagnosis can delay getting the\n",
        "right treatment, increase patient frustration and distrust, and worsen\n",
        "illness, all of which may be disproportionately affecting one subgroup\n",
        "of the population defined by at least two intersecting features that we\n",
        "know of (gender and race).\n",
        "\n",
        "### The hypothetical scenario\n",
        "\n",
        "Based on this finding, we've developed a hypothetical scenario. You\n",
        "should note that this scenario (and the simulated data we're using) is\n",
        "**overly simplified**. But we hope it gives you a sense of how the\n",
        "application of ML can exacerbate biases in training data, and how we can\n",
        "evaluate models for bias from an intersectional perspective.\n",
        "\n",
        "Imagine we have some **electronic health record data on 10,000\n",
        "patients** who have been diagnosed at *Health System A* with either\n",
        "affective disorder (denoted with 0) or schizophrenia (denoted with 1)\n",
        "over the past 10 years. This data contains information on their sex,\n",
        "race, some psychosocial information, and information from clinical\n",
        "assessments on their symptoms. Hospital executives have had a classifier\n",
        "built that will take this information collected about a patient at\n",
        "intake, and **assign a diagnosis of either affective disorder (0) or\n",
        "schizophrenia (1)**. They train a binary classifier on the data to\n",
        "assign new incoming patients a diagnosis, to triage them into the\n",
        "appropriate clinic for treatment.\n",
        "\n",
        "Now, let's say this hospital is run by some executives who would really\n",
        "like to cut costs. They don't want to do any further assessment after\n",
        "classification, and they are **planning to administer treatments to\n",
        "patients following this automated triage.**\n",
        "\n",
        "But first, hospital executives must provide must provide some evidence\n",
        "to stakeholders that this classifier works well in diagnosing patients,\n",
        "so they ask the data science team to collect a test sample of 1000\n",
        "patients, on which they must evaluate the model. The executives argue\n",
        "that the classifier works very well, based on some impressive\n",
        "sensitivity and specificity values. The stakeholders are not fully\n",
        "convinced (thinking that the executives may be a little too eager to get\n",
        "this model deployed), so they hire us (an independent consulting firm)\n",
        "to evaluate the model.\n",
        "\n",
        "Note that this is one overly simplistic component of our case scenario.\n",
        "Typically, much more consideration and evaluation would occur before ML\n",
        "is deployed in a real-life setting, especially in healthcare. When it\n",
        "comes to complicated diagnoses (like schizophrenia), any use of ML is\n",
        "also likely to complement human or clinical judgment.\n",
        "\n",
        "Model Development\n",
        "-----------------\n",
        "\n",
        "Let\\'s build our first machine learning model. First, we need to import\n",
        "the required libraries and data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbLJHmjBjelv"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as skm\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Read in dataset\n",
        "data_openml = fetch_openml(data_id=45040)\n",
        "data = data_openml.data\n",
        "data[\"Diagnosis\"] = data_openml.target\n",
        "\n",
        "# Partition the data into train and test sets\n",
        "X_train = data.loc[data.dataset == \"train\"]\n",
        "X_test = data.loc[data.dataset == \"test\"]\n",
        "\n",
        "X_train = X_train.drop(\"dataset\", axis=1)\n",
        "X_test = X_test.drop(\"dataset\", axis=1)\n",
        "\n",
        "\n",
        "# This function formats the data for stacked bar graphs\n",
        "def grouppivot(labelgroup, yvalue, dataset, SZonly=False):\n",
        "    # Select only columns with a SZ diagnosis\n",
        "    if SZonly:\n",
        "        dataset = dataset.loc[dataset.Diagnosis == 0]\n",
        "\n",
        "    # Group by label group, and normalize by y value within those groups\n",
        "    grouped = (\n",
        "        dataset.groupby([labelgroup])[yvalue]\n",
        "        .value_counts(normalize=True)\n",
        "        .rename(\"percentage\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    pivot = pd.pivot_table(\n",
        "        grouped, index=labelgroup, columns=yvalue, values=\"percentage\", aggfunc=\"sum\"\n",
        "    )\n",
        "    return pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elhbGMt5jelw"
      },
      "source": [
        "Exploring the data\n",
        "==================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCyuzP6ejelw"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDKbpv_jelw"
      },
      "source": [
        "By examining the first few cases, we can broadly get a sense of what the\n",
        "data looks like:\n",
        "\n",
        "-   Diagnosis is binary, with 1 corresponding to schizophrenia, and 0\n",
        "    corresponding to affective disorder.\n",
        "-   We have a binary sex variable, along with a race variable with\n",
        "    Black, Asian, White, and Hispanic as possible values. While\n",
        "    including these in a model seems problematic, we will explore the\n",
        "    problems that arise when they are removed, while also using these\n",
        "    features to conduct an intersectional bias assessment.\n",
        "-   Finally, we have a range of psychosocial and clinical variables that\n",
        "    will help the model to make a prediction\n",
        "\n",
        "You\\'ll notice that this dataset is very clean, with no missing or\n",
        "unexpected values. If we used real-world data from hospital records, it\n",
        "would be much messier, including:\n",
        "\n",
        "-   many missing values, and likely not missing at random (e.g.,\n",
        "    distress, impairment, or language barriers preventing a patient from\n",
        "    being able to answer questions, no resources, or staff available to\n",
        "    help, unwillingness of patients to disclose sensitive information,\n",
        "    such as disability or sexual orientation)\n",
        "-   many unexpected values, potentially due to human or system logging\n",
        "    errors (e.g., a numeric responses for a categorical variable, a\n",
        "    negative value for a variable that must be positive, such as a wait\n",
        "    time)\n",
        "-   variables that are much more complex (e.g., race or ethnicity coded\n",
        "    in a multitude of ways, `maslej2022race`{.interpreted-text\n",
        "    role=\"footcite\"} with many individuals having mixed racial\n",
        "    backgrounds, psychosocial variables never cleanly separate into\n",
        "    *stable* or *unstable* housing or *yes* or *no* for delay (in\n",
        "    reality, these constructs are rarely even captured, and they may be\n",
        "    inferred, e.g., based on location or income)\n",
        "-   subsets of older data not digitally captured\n",
        "\n",
        "Real world data for this type of task would include many more variables\n",
        "and require months of processing and linking, but we are going to use\n",
        "this **simulated** dataset in order to convey the important ideas.\n",
        "\n",
        "Keeping this caveat in mind, let\\'s plot some graphs to better visualize\n",
        "the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l911fUFRjelw"
      },
      "outputs": [],
      "source": [
        "# Format graphs\n",
        "diagnosis = grouppivot(\"dataset\", \"Diagnosis\", data)\n",
        "sex = grouppivot(\"dataset\", \"Sex\", data)\n",
        "race = grouppivot(\"dataset\", \"Race\", data)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
        "diagnosis[[1, 0]].plot.bar(stacked=True, ax=axes[0])\n",
        "\n",
        "axes[0].set_title(\"Diagnosis across train and test sets\")\n",
        "sex.plot.bar(stacked=True, ax=axes[1])\n",
        "\n",
        "axes[1].set_title(\"Sex across train and test sets\")\n",
        "race.plot.bar(stacked=True, ax=axes[2])\n",
        "\n",
        "axes[2].set_title(\"Race across train and test sets\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLYSWxoLjelw"
      },
      "source": [
        "These plots show that the train and test sets have similar proportions\n",
        "of data across diagnosis, sex, and race, so the way our data is\n",
        "partitioned seems fine.\n",
        "\n",
        "We observe a substantially higher proportion of white and Black\n",
        "individuals compared to Asian and Hispanic individuals. If this were\n",
        "real-world data, we might hypothesize about why this would be the case.\n",
        "In what ways could these trends be related to systemic factors, like the\n",
        "underrepresentation of some groups in data collection? However, this is\n",
        "a dataset simulated for our case scenario, and these trends may not\n",
        "appear in real-world health systems.\n",
        "\n",
        "Let\\'s move forward with building our predictive model.\n",
        "\n",
        "Preprocessing\n",
        "=============\n",
        "\n",
        "First, we distinguish our outcome or label (diagnosis) from the training\n",
        "features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bD5OEW8jelx"
      },
      "outputs": [],
      "source": [
        "# Split the data into x (features) and y (diagnosis)\n",
        "y_train = X_train['Diagnosis']\n",
        "X_train = X_train.drop(\"Diagnosis\", axis=1)\n",
        "\n",
        "y_test = X_test['Diagnosis']\n",
        "X_test = X_test.drop(\"Diagnosis\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.loc[0]"
      ],
      "metadata": {
        "id": "0zVocPg3pWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0E6ofzojelx"
      },
      "source": [
        "Next, we do some minimal preprocessing to one hot encode the categorical\n",
        "variables, bearing in mind that preprocessing real-world hospital record\n",
        "data is typically much more laborious.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYo3Nn5rjelx"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "\n",
        "# Perform one hot encoding\n",
        "categories = [\"Sex\", \"Race\", \"Housing\", \"Delay\"]  # Categorial variables\n",
        "\n",
        "\n",
        "# Define a function for one hot encoding\n",
        "def onehot(data, categories=categories):\n",
        "    ordinalencoder = OneHotEncoder()\n",
        "    onehot = ordinalencoder.fit_transform(data[categories])\n",
        "\n",
        "    columns = []\n",
        "    for i, values in enumerate(ordinalencoder.categories_):\n",
        "        for j in values:\n",
        "            columns.append(str(categories[i] + \"_\" + j))\n",
        "\n",
        "    return pd.DataFrame(onehot.toarray(), columns=columns)\n",
        "\n",
        "\n",
        "# Apply transformation to data\n",
        "X_train = X_train.reset_index(drop=True).join(onehot(X_train))\n",
        "X_test = X_test.reset_index(drop=True).join(onehot(X_test))\n",
        "\n",
        "# Drop the original categories\n",
        "X_train = X_train.drop(categories, axis=1)\n",
        "X_test = X_test.drop(categories, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.loc[0]"
      ],
      "metadata": {
        "id": "zXlE2gPRouqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv5eKfpKjelx"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "For this model, we\\'ll use a simple logistic regression model with\n",
        "elastic net for regularization in sklearn across 1000 max iterations.\n",
        "You could still do the same bias assessment we\\'ll be carrying out here\n",
        "with other models, because the approach we will use is a **post-hoc\n",
        "approach**, meaning it only requires the model predictions and not\n",
        "access to the model itself.\n",
        "\n",
        "This is in contrast to some model-specific fairness approaches that\n",
        "require access to the model internals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op4EE205jelx"
      },
      "outputs": [],
      "source": [
        "# Defining a logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty=\"elasticnet\", max_iter=1000, solver=\"saga\", l1_ratio=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RNL5x5pjelx"
      },
      "source": [
        "We train the model and apply it to generate predictions on our test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0HazQRIjelx"
      },
      "outputs": [],
      "source": [
        "# Training the model with all available features\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# generate 10000 predictions for 10000 train individuals\n",
        "train_predictions = model.predict(X_train)\n",
        "print(\"Training accuracy: \", skm.accuracy_score(y_train, train_predictions))  # Training accuracy\n",
        "\n",
        "# generate 1000 predictions for 1000 test individuals\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Test accuracy: \", skm.accuracy_score(y_test, predictions))  # Test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KridlI9_jelx"
      },
      "source": [
        "We notice that the train and test accuracy are all fairly good. We can\n",
        "visualize the performance of the model further by looking at a confusion\n",
        "matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giAup2aGjelx"
      },
      "outputs": [],
      "source": [
        "def confusionmatrix(truelabels, predictions):\n",
        "    confusion_matrix = skm.confusion_matrix(truelabels, predictions)\n",
        "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
        "    print(\n",
        "        \"Sensitivity: \",\n",
        "        tp / (tp + fn),\n",
        "        \"\\nSpecificity: \",\n",
        "        tn / (tn + fp),\n",
        "        \"\\nPPV: \",\n",
        "        tp / (tp + fp),\n",
        "        \"\\nNPV: \",\n",
        "        tn / (tn + fn),\n",
        "    )\n",
        "\n",
        "    skm.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
        "\n",
        "\n",
        "confusionmatrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPCKqYIUjely"
      },
      "source": [
        "Again, we notice very solid performance. The model is correctly\n",
        "identifying positive cases (Sensitivity), while rejecting negative ones\n",
        "(Specificity). The hospital executives probably took a look at the\n",
        "model, and believed that it was ready for deployment.\n",
        "\n",
        "This performance is another overly-simplistic component of our case\n",
        "scenario. In real-world settings, and for psychiatric outcomes in\n",
        "particular, ML rarely achieves performance this good, partly because\n",
        "training features are never as clearly related to outcomes as complex as\n",
        "diagnoses of Schizophrenia or Affective Disorder. These conditions are\n",
        "often heterogeneous, meaning that individuals with the same disorder can\n",
        "have very different symptom profiles. Symptoms of schizophrenia (such as\n",
        "a lack of emotion and motivation, cognitive impairment, and even\n",
        "psychosis) can overlap with symptoms of depression. Another reason ML\n",
        "often falls short in predicting psychiatric outcomes is when these\n",
        "outcomes tend to be rare (such as in the case of suicide\n",
        "`belsher2019prediction`{.interpreted-text role=\"footcite\"}). Although\n",
        "our simulated data is fairly balanced with respect to the two diagnoses,\n",
        "in reality, Schizophrenia is much less common than Affective Disorder.\n",
        "If collected naturally based on different patients seen at a health\n",
        "system, our classes would be very imbalanced.\n",
        "\n",
        "Fairness Assessment\n",
        "===================\n",
        "\n",
        "Many ML algorithms deployed in the past have failed to properly address\n",
        "fairness. Perhaps the most infamous example of this is the COMPAS\n",
        "algorithm `angwin2016machine`{.interpreted-text role=\"footcite\"} for\n",
        "prediction criminal recidivism, where Black defendants were found to\n",
        "have higher risk scores, a higher false positive rate, and a lower false\n",
        "negative rate compared to white defendants. In other applications of ML,\n",
        "the lack of fairness concerns proves pervasive. An audit of three facial\n",
        "recognition software from IBM and Microsoft found the error rate for\n",
        "darker-skinned females to be 34% higher than for lighter-skinned males\n",
        "`buolamwini2018gender`{.interpreted-text role=\"footcite\"}. When\n",
        "translating into a gendered language, Google Translate skews towards\n",
        "masculine translations for words like \"strong\" or \"doctor,\" while\n",
        "skewing feminine for words like \"beautiful\" or \"nurse.\"\n",
        "`kuczmarski2018reducing`{.interpreted-text role=\"footcite\"}\n",
        "\n",
        "These are clear problems which we aim to avoid by performing a proper\n",
        "fairness assessment. Let\\'s do that now.\n",
        "\n",
        "Quantifying fairness\n",
        "--------------------\n",
        "\n",
        "Many fairness metrics are calculated from basic performance metrics,\n",
        "like sensitivity, specificity, TPR, FPR, etc. There are many performance\n",
        "metrics available, so we need to select one that is most relevant to our\n",
        "task: decision trees like\n",
        "[this](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)\n",
        "one provided by Aequitas provide an easy way to understand the use cases\n",
        "for different metrics.\n",
        "\n",
        "As an example, one common (and arguably most successful to date)\n",
        "application of ML in healthcare is in the diagnosis of tumours from\n",
        "medical images (like chest x-rays) as being cancerous or non-cancerous.\n",
        "\n",
        "**In this task, which performance metrics are most relevant?**\n",
        "\n",
        "In this case, **false negative predictions** would probably be most\n",
        "relevant, because when classifying tumours as being cancerous or\n",
        "non-cancerous, we would prefer to mistake a benign tumour as being\n",
        "cancerous, as compared to the other way around. This is called a false\n",
        "positive (or misdiagnosis), and it's better than a false negative (or\n",
        "underdiagnosis) in this specific circumstance.\n",
        "\n",
        "Why? Because a false negative would result in a false perception that\n",
        "the patient does not have cancer, which could delay potentially\n",
        "life-saving treatment.\n",
        "\n",
        "There is a study suggesting that, when diagnosing the nature of tumours\n",
        "from medical images, false negative rates are higher for disadvantaged\n",
        "or underserved groups defined by intersecting features (e.g.,\n",
        "low-income, Black women).\n",
        "`seyyedkalantari2021underdiagnosis`{.interpreted-text role=\"footcite\"}\n",
        "This is an excellent, real-world example of how the application of ML in\n",
        "healthcare can *amplify harms*, if deployed without concern for\n",
        "fairness.\n",
        "\n",
        "*Which performance metric is most important in our case?*\n",
        "\n",
        "In our case, the opposite would be true - a false positive or\n",
        "misdiagnosis is more harmful for our hypothetical scenario, because it\n",
        "would lead to unnecessary treatment for schizophrenia being\n",
        "administered, as well as other negative impacts of a schizophrenia\n",
        "diagnosis, such as stigma or a poorer perceived prognosis. While we want\n",
        "every patient to be appropriately diagnosed, clinicians generally agree\n",
        "that it is better to be misdiagnosed with an affective disorder as\n",
        "compared to schizophrenia. Indeed, when diagnosing patients, clinicians\n",
        "tend to rule out any factors that could lead to psychosis or symptoms of\n",
        "schizophrenia, such as trauma, substance use, and affective disorder,\n",
        "which in some cases, can have psychotic features.\n",
        "\n",
        "So in our example, we'd like to evaluate whether rates of misdiagnosis\n",
        "or **false positives** are the same across patient groups. False\n",
        "positive rates are calculated by dividing false positive predictions by\n",
        "all negative predictions ($FPR = \\frac{FP}{FP + TN}$)\n",
        "\n",
        "Fairness metrics\n",
        "----------------\n",
        "\n",
        "When evaluating ML models for fairness, we typically examine the ratio\n",
        "of a given performance metric between two groups of interest, and\n",
        "whether it is greater or less than true parity (1). The numerator in\n",
        "this equation is the metric for the group we are interested in\n",
        "evaluating (on top), and the denominator is the same metric for the\n",
        "reference group (bottom).\n",
        "\n",
        "> $\\text{Relative Metric} = \\frac{\\text{Metric }_{\\text{group of interest}}}{\\text{Metric\n",
        "> }_{\\text{reference group}}}$\n",
        "\n",
        "If this ratio is greater than 1, then the metric is higher in the group\n",
        "of interest vs reference.\n",
        "\n",
        "If this ratio is less than 1, then the metric is lower in the group of\n",
        "interest vs reference.\n",
        "\n",
        "Fairness metrics themselves can be broadly divided into 3 categories\n",
        "`barocas2019fairness`{.interpreted-text role=\"footcite\"}:\n",
        "\n",
        "-   Independence: Outcomes should be evenly distributed between\n",
        "    subgroups\n",
        "-   Separation: Errors should be evenly distributed between subgroups\n",
        "-   Sufficiency: Given a prediction, an individual has an equal\n",
        "    likelihood of belonging to any subgroup\n",
        "\n",
        "In our case, we calculate the relative false positive rate via the\n",
        "category of separation, by comparing false positive or misdiagnosis\n",
        "rates between our group of interest (Black men) and our reference group\n",
        "(white men).\n",
        "\n",
        "> $\\text{Relative FPR} = \\frac{\\text{FPR}_{\\text{Black men}}}{\\text{FPR}_{\\text{white\n",
        "> men}}}$\n",
        "\n",
        "Note that there are some subjective decisions we are making in our\n",
        "fairness assessment. First, we have chosen white men as our reference\n",
        "group, but other groups might be justified here. For instance, if we are\n",
        "interested in racialization related to Black groups specifically, we\n",
        "might choose another racialized group as our reference (e.g., Hispanic\n",
        "men). If we are interested in the impact of gender, we may choose Black\n",
        "women as our reference group.\n",
        "\n",
        "Another decision is related to the metric -- we could have also\n",
        "considered some other performance metrics. Selection rate, for example,\n",
        "refers to the proportion of positive predictions between the two groups:\n",
        "\n",
        "> $\\text{Selection rate}= \\frac{TP + FP}{n}$\n",
        "\n",
        "Because we are primarily interested in misdiagnosis of schizophrenia and\n",
        "not the total number of schizophrenia diagnoses (which indeed may be\n",
        "higher in certain demographic groups), false positive rate is likely a\n",
        "better fit for our task.\n",
        "\n",
        "Critically, there are no readily-agreed upon definitions and\n",
        "understandings of fairness. There are over 70 definitions of fairness,\n",
        "many of which conflict with each other, making it **impossible to\n",
        "simultaneously satisfy all possible metrics for fairness.**\n",
        "\n",
        "Racial bias\n",
        "-----------\n",
        "\n",
        "In order to perform a fairness assessment, there are a few key things we\n",
        "need to do:\n",
        "\n",
        "**1) Which population might be unfairly affected by this model?**\n",
        "\n",
        "Based on the research, we have defined this group to be Black men. In\n",
        "other words, we are interested in a particular intersection of race and\n",
        "sex. In fairness terms, we define race and sex to be **sensitive\n",
        "variables** - features that we want to ensure our model isn\\'t being\n",
        "discriminatory against.\n",
        "\n",
        "We\\'ll start with an assessment purely on the basis of Race as a\n",
        "sensitive variable (How might all Black individuals be affected by an\n",
        "unfair model?) and then add in sex (How might Black men be affected by\n",
        "unfair model?) in order to demonstrate the value of an intersectional\n",
        "approach.\n",
        "\n",
        "**2) What is fairness in this context?**\n",
        "\n",
        "We\\'ve also determined a fairness metric, which quantifies the exact\n",
        "nature of the discrimination which groups may face and that we seek to\n",
        "minimize. As we mentioned, various fairness metrics may be relevant, but\n",
        "we are primarily concerned about misdiagnosing individuals with\n",
        "affective disorder as having schizophrenia. We examine false positive\n",
        "rates (i.e., false diagnoses of schizophrenia), with the help of\n",
        "Fairlearn.\n",
        "\n",
        "**3) How do we compare populations?**\n",
        "\n",
        "Finally, the performance of a ML model cannot be analyzed in isolation,\n",
        "but rather in comparison between different demographic subgroups. In\n",
        "order to do this, a reference group is required.\n",
        "\n",
        "As we explained, we will use white individuals as a reference group, but\n",
        "this assessment could be conducted with any reference group, or compared\n",
        "to an overall score across all groups. It is important to note that both\n",
        "approaches can be problematic: there are concerns that using white\n",
        "groups as reference groups centers their perspectives or experiences and\n",
        "represents other groups as *other* or outliers, or that using overall\n",
        "averages may mask disparities (especially intersectional ones) and\n",
        "ignore important historical and social context. Keeping this in mind, we\n",
        "should carefully consider which group we use as a reference and the\n",
        "implications of this choice.\n",
        "\n",
        "Here, we apply Fairlearn to compare performance in the test set among\n",
        "the different racial groups, with a focus on evaluating false positive\n",
        "rate ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairlearn"
      ],
      "metadata": {
        "id": "_dO6CLGpwNEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKOriUzujely"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame, false_positive_rate\n",
        "\n",
        "test = data.loc[data.dataset == \"test\"]\n",
        "\n",
        "def f(truelabels, predictions):\n",
        "    # Define race to be the sensitive variable\n",
        "    sensitive = test['Race']\n",
        "\n",
        "    # Define a MetricFrame using a FPR of the defined sensitive features, using the true labels and\n",
        "    # predictions\n",
        "    fmetrics = MetricFrame(\n",
        "        metrics=false_positive_rate,\n",
        "        y_true=truelabels,\n",
        "        y_pred=predictions,\n",
        "        sensitive_features=sensitive,\n",
        "    )\n",
        "\n",
        "    # Compute the Relative FPR relative to white individuals.\n",
        "    results = pd.DataFrame(\n",
        "        [fmetrics.by_group, fmetrics.by_group / fmetrics.by_group.White],\n",
        "        index=[\"FPR\", \"Relative FPR\"],\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "f(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHcIMA-Bjely"
      },
      "source": [
        "In the table above, the first row shows the metric (false positive\n",
        "rates), while the second row shows the ratio of the metric between\n",
        "specified group and reference. These values are all expressed as\n",
        "percentages.\n",
        "\n",
        "Some observations to note:\n",
        "\n",
        "-   The FPR are fairly low across the board, which are due to our use of\n",
        "    simulated data (as we explained, false positive rates in real-world\n",
        "    data are likely to be much higher)\n",
        "-   However, **disparities in performance are emerging**: Black patients\n",
        "    have a relative FPR of 2.9, which means they are being misdiagnosed\n",
        "    with schizophrenia when they have affective disorder at a rate that\n",
        "    is 2.9x higher than those who are white. This is quite concerning.\n",
        "-   We see that Hispanic individuals are also poorly affected by this\n",
        "    model, with a relative FPR of 1.3 (but you should note that this is\n",
        "    not an effect that has been explicitly noted in the literature and\n",
        "    is likely an artifact of our simulated data)\n",
        "-   The white population has a relative FPR of 1 - this makes since they\n",
        "    are our reference population, and other other group is being\n",
        "    compared against the FPR for whites.\n",
        "\n",
        "Intersectional bias\n",
        "===================\n",
        "\n",
        "However, we suspect this bias might only extend to identities defined by\n",
        "the intersecting features of sex and race (i.e., Black men\n",
        "`gara2019naturalistic`{.interpreted-text role=\"footcite\"}).\n",
        "\n",
        "Let\\'s repeat this assessment then.\n",
        "\n",
        "To do this with Fairlearn, we define sensitive features as the\n",
        "intersection of race and sex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnP2fLZVjely"
      },
      "outputs": [],
      "source": [
        "def intersectionalf(truelabels, predictions):\n",
        "    # Sensitive features are now the intersection of race and sex\n",
        "    sensitive = pd.DataFrame(np.stack([test['Race'], test['Sex']], axis=1), columns=[\"Race\", \"Sex\"])\n",
        "\n",
        "    fmetrics = MetricFrame(\n",
        "        metrics=false_positive_rate,\n",
        "        y_true=truelabels,\n",
        "        y_pred=predictions,\n",
        "        sensitive_features=sensitive,\n",
        "    )\n",
        "\n",
        "    results = pd.DataFrame(\n",
        "        [fmetrics.by_group, fmetrics.by_group / fmetrics.by_group.White.Male],\n",
        "        index=[\"FPR\", \"Relative FPR\"],\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "intersectionalf(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeBokO3Ujely"
      },
      "source": [
        "What do we notice?\n",
        "\n",
        "-   Both the FPR and Relative FPR of Black men is much higher than that\n",
        "    of Black women and men combined. Black men are now misdiagnosed at a\n",
        "    rate that is almost 5.18x that of White men. This suggests that\n",
        "    Black men are specifically being unfairly treated by this model.\n",
        "-   Black women still have a higher rate of misdiagnosis than white men,\n",
        "    but we see now that the Relative FPR for this group is actually\n",
        "    lower than the Black population as a whole. This provides further\n",
        "    support for the notion that Black men are an intersectional group\n",
        "    that is being unfairly harmed.\n",
        "-   Crucially, this insight is something we would have completely missed\n",
        "    out on without looking at this problem through an intersectional\n",
        "    lens\n",
        "\n",
        "We can take a look at the data to get an idea about what might be going\n",
        "on. First, we examine the frequency of diagnoses, stratified by\n",
        "intersectional groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFc-gPKojely"
      },
      "outputs": [],
      "source": [
        "train = data.loc[data.dataset == \"train\"]\n",
        "m = sns.FacetGrid(train, row=\"Sex\", col=\"Race\")\n",
        "m.map(sns.histplot, \"Diagnosis\", discrete=True, shrink=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg5yw_Cgjely"
      },
      "source": [
        "We see that the frequency of the two diagnoses (of affective disorder or\n",
        "0 and schizophrenia or 1) are fairly similar among all intersectional\n",
        "subgroups, except Black men, who have a much higher rate of\n",
        "schizophrenia diagnosis.\n",
        "\n",
        "Fairness through Unawareness?\n",
        "=============================\n",
        "\n",
        "In our data, Black men are less likely to be diagnosed with affective\n",
        "disorder, and more likely to be diagnosed with schizophrenia than other\n",
        "groups (and almost two times as likely as white men). Perhaps the model\n",
        "is picking up on this trend, which is contributing to bias. If we remove\n",
        "race, the classifier will no longer have access to this information\n",
        "during training.\n",
        "\n",
        "This is an approach that is commonly termed **fairness through\n",
        "unawareness.** Specifically, it refers to the notion that a model that\n",
        "does not have access to a given feature when making predictions cannot\n",
        "be unfair with respect to that feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMErGitcjely"
      },
      "source": [
        "Define and drop race-related variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNHX-vRnjely"
      },
      "outputs": [],
      "source": [
        "race_cat = [\"Race_Asian\", \"Race_Black\", \"Race_Hispanic\", \"Race_White\"]\n",
        "trainx_norace = X_train.drop(race_cat, axis=1)\n",
        "testx_norace = X_test.drop(race_cat, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRiA650Djelz"
      },
      "source": [
        "Now that we\\'ve dropped the variables, we\\'ll train a second model that\n",
        "is identical to the first one, except it no longer uses Race as a\n",
        "feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhG1hOdGjelz"
      },
      "outputs": [],
      "source": [
        "# Define and train a second model\n",
        "model2 = LogisticRegression(penalty=\"elasticnet\", max_iter=1000, solver=\"saga\", l1_ratio=1)\n",
        "model2 = model2.fit(trainx_norace, y_train)\n",
        "\n",
        "train_predictions = model2.predict(trainx_norace)\n",
        "print(\"Training accuracy: \", skm.accuracy_score(y_train, train_predictions))  # Training accuracy\n",
        "\n",
        "predictions = model2.predict(testx_norace)\n",
        "print(\"Test accuracy: \", skm.accuracy_score(y_test, predictions))  # Test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l9PDf8Sjelz"
      },
      "source": [
        "We\\'ll note that the accuracy is still solid. It has decreased slightly,\n",
        "which makes sense given that our model has access to fewer features with\n",
        "which to maximize predictive accuracy.\n",
        "\n",
        "That said, our most important objective is to analyze whether fairness\n",
        "has increased as a result of our change, so let\\'s perform another bias\n",
        "assessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PncyP-_jelz"
      },
      "outputs": [],
      "source": [
        "f(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6Balhtjelz"
      },
      "source": [
        "Oh yikes - by removing race from our model, the FPR for the Black\n",
        "population has increased to 3.8x that of our reference group.\n",
        "\n",
        "Does intersectional fairness show a similar trend?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAOGc_qojelz"
      },
      "outputs": [],
      "source": [
        "intersectionalf(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXnAxxr9jelz"
      },
      "source": [
        "If we analyze this problem through an intersectional lens, we notice\n",
        "that the relative FPR has increased even further, and Black men are now\n",
        "being misdiagnosed at a rate that is 7x higher than white men.\n",
        "\n",
        "**After we have observed this finding, is it a good idea to implement\n",
        "our model? Probably not.**\n",
        "\n",
        "After all, the potential harm related to an unfair model like this one\n",
        "does not stop at the fairness assessment - it can manifest in impactful\n",
        "and pervasive ways in the systems where it is potentially deployed. A\n",
        "Black man misdiagnosed with schizophrenia with the help of a ML model\n",
        "may become more distrustful towards healthcare in the future. At any\n",
        "future visits, this distrust might manifest in certain types of\n",
        "behaviours (e.g., an increased sense of tension) that could be\n",
        "interpreted as further evidence of schizophrenia, ultimately\n",
        "contributing to further misdiagnosis. This type of feedback is extremely\n",
        "detrimental, as **algorithms reinforce and propagate the unfairness**\n",
        "encoded within the data that is representative of society\\'s own\n",
        "discriminatory practices.\n",
        "\n",
        "This begs the question though - if unfair predictions made by an\n",
        "algorithm are capable of causing harm, should we attempt to remove this\n",
        "bias from our model? After all, there are methods that exist that allow\n",
        "us to transform our data or modify our training algorithm in a way that\n",
        "would make our model fairer with respect to the fairness metrics we are\n",
        "using.\n",
        "\n",
        "In answering this, it\\'s important to remember that the bias we are\n",
        "seeing in our model are a reflection of systemic biases that exist in\n",
        "real life. While the real life biases that lead to patient misdiagnosis\n",
        "are certainly problematic, detecting these biases in our model isn\\'t\n",
        "necessarily a bad thing. In this case scenario, where our main goal is\n",
        "actually to better understand the model, the presence and quantification\n",
        "of these biases are actually very helpful because they enable us to\n",
        "understand the systemic biases that have been encoded into our data.\n",
        "**In other words, our model can be insightful \\*because\\* it captures\n",
        "these harmful real-world biases.**\n",
        "\n",
        "To this end, attempting to remove the biases from our dataset would be\n",
        "detrimental to our aim. Instead, we should dig a little bit deeper to\n",
        "reflect and analyze some of the systemic factors that could be\n",
        "underlying our findings.\n",
        "\n",
        "Why is our model still biased?\n",
        "==============================\n",
        "\n",
        "Why has removing race information from our training data not fixed our\n",
        "problem?\n",
        "\n",
        "Recall that there is much evidence to support **diagnostic bias**, or\n",
        "the tendency for clinicians to under-emphasize depressive symptoms and\n",
        "over-emphasize psychotic symptoms when assessing black men. But one\n",
        "interesting 2004 study `arnold2004ethnicity`{.interpreted-text\n",
        "role=\"footcite\"} shows that this effect is not necessarily due to the\n",
        "appearance of race. In this study, the researchers examined whether\n",
        "blinding clinicians to a patient's race would remove this diagnostic\n",
        "bias. Black and white patients presenting to the hospital with psychosis\n",
        "were evaluated with structured rating scales, and this evaluation was\n",
        "transcribed. Cues indicating the patient's race were removed from the\n",
        "transcription. Regardless of whether clinicians were blinded or\n",
        "unblinded to the patient's race, they still rated the black men as\n",
        "having increased psychotic symptoms.\n",
        "\n",
        "**So what is going on here?**\n",
        "\n",
        "It's likely that race is **associated with other factors that are\n",
        "relevant to misdiagnosis.** The diagnostic bias (or a tendency to\n",
        "overemphasize symptoms of schizophrenia and under-emphasize depressive\n",
        "symptoms) may be related to socio-environmental factors, for example,\n",
        "Black men with depression facing more barriers to receiving mental\n",
        "healthcare, which results in more severe illness when finally assessed.\n",
        "Growing up in low-income or stressful environments, or early exposure to\n",
        "malnourishment and trauma, can also lead to more severe impairment in\n",
        "daily and cognitive functioning. Black patients may additionally face\n",
        "racialization and poor treatment in health settings, leading them to\n",
        "exhibit paranoia, tension, or distrust at assessment (especially if\n",
        "being assessed by a white clinician). The diagnostic instruments we have\n",
        "are also likely culturally biased, having been developed on mostly white\n",
        "populations, making it difficult to pick up on symptoms of depression in\n",
        "Black patients or men, in particular. (This article\n",
        ":footcite:ct:\\`herbst2022schizophrenia\\`presents a nice overview of\n",
        "these various concerns)\n",
        "\n",
        "Unfortunately then, it's not as simple as removing race from the\n",
        "equation, because of these **persisting systemic biases which are\n",
        "related to misdiagnosis.** And these biases are reflected in the\n",
        "training data, since the socio-environmental and clinical factors\n",
        "relevant for misdiagnosis are also associated with race. The model is\n",
        "picking up on these associations, despite not having access to the race\n",
        "of each patient.\n",
        "\n",
        "Now that we have evidence for intersectional bias in our model, we could\n",
        "explore some of the training features that might underlie this bias. In\n",
        "fact, there is a very recent study by\n",
        ":footcite`banerjee2021readiang`{.interpreted-text role=\"ct\"} which takes\n",
        "this point a bit further. In this study, researchers found that deep\n",
        "learning models trained on medical images, like chest X-rays, performed\n",
        "well at predicting the patient's race, despite not having access to any\n",
        "racial information. The researchers examined some reasonable\n",
        "explanations for how this was even possible, such as minor anatomical\n",
        "differences between racial groups, cumulative effects of racialization\n",
        "or stress, and even image quality between health systems, and none were\n",
        "supported. So there must be some way that models pick up on race that\n",
        "not even us humans can understand.\n",
        "\n",
        "Feature evaluation\n",
        "==================\n",
        "\n",
        "In sum, the tendency for Black men to be misdiagnosed with schizophrenia\n",
        "is not simply a result of clinician or interpersonal bias, but likely\n",
        "reflects systemic factors (e.g., barriers to care leading to severe\n",
        "illness at assessment, expression of emotional and cognitive symptoms of\n",
        "depression, experiences of racialization leading to greater paranoia or\n",
        "distrust).\n",
        "\n",
        "These factors may be reflected in other features in the simulated data,\n",
        "which are related to race, and contribute to bias.\n",
        "\n",
        "We can explore relations among the features in different ways, but one\n",
        "option is to see how various features are related to schizophrenia in\n",
        "the training set, and then explore these features in groups with\n",
        "affective disorder in the test set.\n",
        "\n",
        "The choice of which features to consider is subjective and can be based\n",
        "on existing research or empirical observation (or both). In our case,\n",
        "we\\'ll use the first few features identified as being important for\n",
        "prediction in our ML model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_kVgJWDjelz"
      },
      "source": [
        "Gets the weights associated with each feature, and scales them from\n",
        "0-100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc-6WYZ5jel0"
      },
      "outputs": [],
      "source": [
        "weights = pd.DataFrame(model.coef_[0], X_train.columns, columns=[\"Weight\"])\n",
        "scaler = MinMaxScaler((0, 100))\n",
        "scaled_weights = pd.DataFrame(\n",
        "    scaler.fit_transform(abs(weights)), X_train.columns, columns=weights.columns\n",
        ")\n",
        "scaled_weights.sort_values(by=[\"Weight\"], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC2zoWAkjel0"
      },
      "source": [
        "When we examine features that are most important for prediction in our\n",
        "first model (i.e., the model that included race), it indeed shows that\n",
        "although race is among the important features, there are also other\n",
        "features related to clinical presentation that are contributing to\n",
        "predicting the diagnosis or outcome.\n",
        "\n",
        "As mentioned, we will probe into potential factors underlying\n",
        "misdiagnosis of individuals with affective disorder by examining how\n",
        "these features are related to diagnosis in the training set. Then, we\n",
        "will compare individuals with affective disorder on these features in\n",
        "the test set, stratified by intersectional group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GhRMg2wjel0"
      },
      "outputs": [],
      "source": [
        "# Reloading the data for analysis\n",
        "train = data.loc[data.dataset == \"train\"].drop(\"dataset\", axis=1)\n",
        "test = data.loc[data.dataset == \"test\"].drop(\"dataset\", axis=1)\n",
        "\n",
        "train[\"Diagnosis\"] = train[\"Diagnosis\"].astype(int)\n",
        "test[\"Diagnosis\"] = test[\"Diagnosis\"].astype(int)\n",
        "\n",
        "# Create new intersect column for plotting\n",
        "test[\"intersect\"] = np.nan\n",
        "test.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"White\"), \"intersect\"] = \"WhiteM\"\n",
        "test.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Black\"), \"intersect\"] = \"BlackM\"\n",
        "test.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Hispanic\"), \"intersect\"] = \"HispanicM\"\n",
        "test.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Asian\"), \"intersect\"] = \"AsianM\"\n",
        "test.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"White\"), \"intersect\"] = \"WhiteF\"\n",
        "test.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Black\"), \"intersect\"] = \"BlackF\"\n",
        "test.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Hispanic\"), \"intersect\"] = \"HispanicF\"\n",
        "test.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Asian\"), \"intersect\"] = \"AsianF\"\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
        "sns.barplot(x=\"Diagnosis\", y=\"Rumination\", data=train, ax=axes[0])\n",
        "axes[0].set_title(\"Rumination vs Diagnosis (train)\")\n",
        "\n",
        "sns.barplot(\n",
        "    x=\"intersect\",\n",
        "    y=\"Rumination\",\n",
        "    data=test.loc[test.Diagnosis == 0],\n",
        "    ax=axes[1],\n",
        "    order=[\"BlackF\", \"WhiteF\", \"HispanicF\", \"AsianF\", \"BlackM\", \"WhiteM\", \"HispanicM\", \"AsianM\"],\n",
        ")\n",
        "axes[1].set_title(\"Rumination vs Intersect across groups with AD (test)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYtG9sehjel0"
      },
      "source": [
        "The first graph (top) shows average rumination scores stratified by\n",
        "diagnosis. Rumination is a cognitive feature of depression, referring to\n",
        "repetitive, persistent thinking about the depressive episode. As you can\n",
        "see, rumination is more common in affective disorder than schizophrenia,\n",
        "which is consistent with clinical trends.\n",
        "\n",
        "The second graph (bottom) shows rumination among individuals with\n",
        "affective disorder in the test set. You can see that rumination is lower\n",
        "among men than women, but also that **Black men as compared to white men\n",
        "are less likely to report rumination**, which are the two groups we\\'re\n",
        "comparing. As such, this could be a potential factor contributing to\n",
        "misdiagnosis, leading to higher false positive rates in Black men. Of\n",
        "course, other groups may have lower rumination scores relative to white\n",
        "men as well (e.g., Hispanic men), but the model is picking up on trends\n",
        "related to a variety of features, so we can take a look at one more.\n",
        "\n",
        "Another important feature is tension, which is a symptom of\n",
        "schizophrenia. We can carry out a similar exploration to examine how\n",
        "tension is reported by individuals with affective disorder defined by\n",
        "intersecting features of sex and race, as compared to schizophrenia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYY8timyjel0"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
        "sns.barplot(x=\"Diagnosis\", y=\"Tension\", data=train, ax=axes[0])\n",
        "axes[0].set_title(\"Tension vs Diagnosis (train)\")\n",
        "\n",
        "sns.barplot(\n",
        "    x=\"intersect\",\n",
        "    y=\"Tension\",\n",
        "    data=test.loc[test.Diagnosis == 0],\n",
        "    ax=axes[1],\n",
        "    order=[\"BlackF\", \"WhiteF\", \"HispanicF\", \"AsianF\", \"BlackM\", \"WhiteM\", \"HispanicM\", \"AsianM\"],\n",
        ")\n",
        "axes[1].set_title(\"Tension vs Intersect across groups with AD (test)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUknaCEzjel0"
      },
      "source": [
        "In the training set, people with schizophrenia are more likely to report\n",
        "tension at their clinical assessment (top). Again, looking at how\n",
        "Black men with affective disorder compare to White men on this\n",
        "particular feature in the test set, they are more likely to report\n",
        "tension at the clinical interview. But it\\'s interesting to see that\n",
        "other groups also report high tension relative to our reference group\n",
        "(white men).\n",
        "\n",
        "So it\\'s not the only factor that\\'s potentially contributing to\n",
        "misdiagnosis, but it could be one of the factors that explain the high\n",
        "false positive rates in Black men with affective disorder (as well as in\n",
        "Asian men or Hispanic women, for example).\n",
        "\n",
        "The goal of this tutorial was to show one potential way to probe into\n",
        "factors potentially underlying biased performance, but there are other\n",
        "ways, many of which are not limited to quantitative means. For example,\n",
        "ethnographic techniques can provide a more comprehensive and meaningful\n",
        "understanding of the social, systemic, and political factors that\n",
        "contribute to inequities, and they can be particularly powerful at\n",
        "elucidating the contextual factors leading to training data bias.\n",
        "\n",
        "Conclusions\n",
        "===========\n",
        "\n",
        "Reporting to our stakeholders\n",
        "-----------------------------\n",
        "\n",
        "Getting back to our hypothetical scenario, what do we report back to our\n",
        "stakeholders following our intersectional bias assessment?\n",
        "\n",
        "We conclude that although the model shows excellent performance overall,\n",
        "it may underserve certain demographic groups, like Black men, and it\n",
        "should not be deployed without further assessment by clinicians and\n",
        "further research into the factors contributing to bias. The hospital\n",
        "should consider some targeted interventions (e.g., further assessment\n",
        "for Black men and other intersectional groups, especially those\n",
        "reporting or displaying less rumination or more tension at clinical\n",
        "assessments).\n",
        "\n",
        "Overall, the model should not be deployed without further assessment by\n",
        "clinicians or intervention.\n",
        "\n",
        "Some final points\n",
        "-----------------\n",
        "\n",
        "Researchers have developed algorithmic methods to mitigate the\n",
        "fairness-related harms that may result from ML models (by adjusting\n",
        "model parameters or modifying training data), **but this does not do\n",
        "anything to address the systemic factors** contributing to bias. For\n",
        "example, if we fix our model to reduce false positive predictions in\n",
        "Black men, will this increase their access to care or treatment? Will it\n",
        "help clinicians better differentiate between symptoms of depression or\n",
        "schizophrenia in these groups? As we have demonstrated in our case\n",
        "scenario, the problem is more nuanced and the solution is much more\n",
        "complex, **requiring collaboration between researchers, clinicians, and\n",
        "public health or policy administrators**. We need more research into\n",
        "these issues and interventions that can address them. For example, some\n",
        "evidence suggests that forcing clinicians to carry out more consistent\n",
        "and structured assessments can reduce diagnostic bias (though not\n",
        "completely). Put differently, machine learning systems are of a\n",
        "**sociotechnical nature** (see also our\n",
        "`user guide<concept_glossary>`{.interpreted-text role=\"ref\"} and\n",
        ":footcite`selbst2019fairness`{.interpreted-text role=\"ct\"}).\n",
        "\n",
        "The point is **not** that we shouldn't be using ML to automate or inform\n",
        "clinical tasks (this will likely happen whether we like it or not).\n",
        "Rather, ML can potentially help us **better understand the potential\n",
        "health inequities present within a health system** (many of which we\n",
        "might not catch because our own biases can prevent us from seeing and\n",
        "thinking about the inequities). This underscores the potential of ML to\n",
        "identify contributing features that warrant more research and to improve\n",
        "current clinical practices.\n",
        "\n",
        "References\n",
        "==========\n",
        "\n",
        "::: {.footbibliography}\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}