{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwq_ZK35q9H5"
      },
      "source": [
        "# LIME - Local Interpretable Model-Agnostic Explanations\n",
        "\n",
        "This assignment uses LIME, a package that provides \"Locally Interpetable Model-agnostic Explanations\" for machine learning models.\n",
        "\n",
        "- **Locally interpretable**: for each specific prediction, we can provide a relevant explanation.\n",
        "- **Model-agnostic**: we can provide the same sort of explanation for models of different classes.\n",
        "\n",
        "For more documentation of the LIME package, see [here](https://github.com/marcotcr/lime) and [here](https://lime-ml.readthedocs.io/en/latest/index.html). The original paper describing the method is [here](https://arxiv.org/pdf/1602.04938.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P5Cca7_drV8g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.11.4)\n",
            "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
            "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.4/99.8 MB 7.4 MB/s eta 0:00:14\n",
            "    --------------------------------------- 1.4/99.8 MB 18.2 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 3.1/99.8 MB 24.9 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 4.5/99.8 MB 28.9 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 6.0/99.8 MB 31.9 MB/s eta 0:00:03\n",
            "   -- ------------------------------------- 7.3/99.8 MB 31.3 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 8.9/99.8 MB 31.5 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 10.1/99.8 MB 30.8 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 11.5/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 13.5/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 14.8/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 16.2/99.8 MB 34.4 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 17.6/99.8 MB 34.4 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 19.0/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 20.6/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 21.8/99.8 MB 34.4 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 23.3/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 25.2/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 25.3/99.8 MB 36.4 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 28.0/99.8 MB 36.4 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 30.1/99.8 MB 38.5 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 31.4/99.8 MB 38.6 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 32.9/99.8 MB 38.6 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 34.3/99.8 MB 36.4 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 34.9/99.8 MB 36.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 34.9/99.8 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 36.2/99.8 MB 34.6 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 37.5/99.8 MB 31.2 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 39.2/99.8 MB 29.7 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 40.4/99.8 MB 28.5 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 42.0/99.8 MB 28.5 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 43.4/99.8 MB 28.4 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 44.7/99.8 MB 29.7 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 46.0/99.8 MB 36.3 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 47.5/99.8 MB 36.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 49.0/99.8 MB 36.4 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 50.7/99.8 MB 36.3 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 52.6/99.8 MB 38.5 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 54.5/99.8 MB 40.9 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 56.1/99.8 MB 40.9 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 58.2/99.8 MB 40.9 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 59.8/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 61.3/99.8 MB 38.6 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 63.5/99.8 MB 38.6 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 65.0/99.8 MB 38.5 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 66.4/99.8 MB 38.6 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 68.0/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 69.7/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 71.4/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 71.9/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 72.2/99.8 MB 32.8 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 72.5/99.8 MB 28.5 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 74.0/99.8 MB 28.5 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 75.2/99.8 MB 28.5 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 76.6/99.8 MB 27.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 78.2/99.8 MB 27.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 79.6/99.8 MB 27.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 81.4/99.8 MB 28.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 83.1/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 84.6/99.8 MB 38.5 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 86.3/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 88.3/99.8 MB 43.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 90.1/99.8 MB 43.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 91.6/99.8 MB 43.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 92.7/99.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 94.5/99.8 MB 43.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 96.2/99.8 MB 43.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 96.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 97.2/99.8 MB 32.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  98.6/99.8 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  99.7/99.8 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  99.7/99.8 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  99.7/99.8 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  99.7/99.8 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 99.8/99.8 MB 21.8 MB/s eta 0:00:00\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install lime # if you are running on JupyterLab, use !/opt/anaconda/bin/pip\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_sEEdxC0q9IK"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import sklearn.datasets\n",
        "import sklearn.ensemble\n",
        "import sklearn.preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from lime import submodular_pick\n",
        "import xgboost\n",
        "from xgboost import plot_importance\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR1b4tyQq9IM"
      },
      "source": [
        "***\n",
        "## Part 1: Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhM5lhswq9IM"
      },
      "source": [
        "We will be using the Adult dataset which we can use to predict whether a person makes over 50K dollars per year based on census information. A version of the dataset is found on UCI ML Repository : [here](https://archive.ics.uci.edu/ml/datasets/adult)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV9s8V2iq9IO"
      },
      "source": [
        "### Read in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mx0PKgRUq9IO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape:  (32561, 15)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>Education</th>\n",
              "      <th>Education-Num</th>\n",
              "      <th>Marital-Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital-Gain</th>\n",
              "      <th>Capital-Loss</th>\n",
              "      <th>Hours-per-week</th>\n",
              "      <th>Country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age          Workclass  fnlwgt   Education  Education-Num  \\\n",
              "0   39          State-gov   77516   Bachelors             13   \n",
              "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
              "2   38            Private  215646     HS-grad              9   \n",
              "3   53            Private  234721        11th              7   \n",
              "4   28            Private  338409   Bachelors             13   \n",
              "\n",
              "        Marital-Status          Occupation    Relationship    Race      Sex  \\\n",
              "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
              "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
              "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
              "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
              "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
              "\n",
              "   Capital-Gain  Capital-Loss  Hours-per-week         Country  income  \n",
              "0          2174             0              40   United-States   <=50K  \n",
              "1             0             0              13   United-States   <=50K  \n",
              "2             0             0              40   United-States   <=50K  \n",
              "3             0             0              40   United-States   <=50K  \n",
              "4             0             0              40            Cuba   <=50K  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read in the data\n",
        "colnames = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\",\n",
        "            \"Marital-Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\",\n",
        "            \"Capital-Gain\", \"Capital-Loss\",\"Hours-per-week\", \"Country\",\n",
        "            \"income\"]\n",
        "data_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
        "                      names = colnames)\n",
        "print(\"Shape: \", data_df.shape)\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl6mTdX2q9IQ"
      },
      "source": [
        "### Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "buhCTvcuq9IR"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['Education-Num'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO - Drop Education-Num, which has a functional dependency with Education\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEducation-Num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, data_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m data_df\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[1;32mc:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5346\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5350\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5351\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[1;32mc:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['Education-Num'] not found in axis\""
          ]
        }
      ],
      "source": [
        "# TODO - Drop Education-Num, which has a functional dependency with Education\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpadUNmNq9IT"
      },
      "outputs": [],
      "source": [
        "# Get a list of feature names (excluding the outcome variable)\n",
        "feature_names = data_df.columns[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNtzudwOq9IT"
      },
      "outputs": [],
      "source": [
        "# Mark labels and encode them using sklearn\n",
        "labels = data_df.iloc[:,-1]\n",
        "le= sklearn.preprocessing.LabelEncoder()\n",
        "le.fit(labels)\n",
        "labels = le.transform(labels)\n",
        "class_names = le.classes_\n",
        "data = data_df.iloc[:,:-1]\n",
        "le_label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Class names: \", class_names)\n",
        "print(\"Label mapping: \", le_label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w56XCNeq9IU"
      },
      "outputs": [],
      "source": [
        "# Check if there are categorical variables that we need to make dummies for\n",
        "print(data.dtypes)\n",
        "# Get a list of which variables are categorical\n",
        "categorical_features  = [i for i in range(len(data.dtypes)) if data.dtypes[i]=='object']\n",
        "print(\"Indices of categorical features: \", categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLCXpuzhq9IW"
      },
      "source": [
        "LIME will require us to provde categorical variables as a single column, not as dummies, so we can't just explode these columns the way we normally would during pre-processing.\n",
        "\n",
        "Instead, we'll use some sklearn tools to take the following steps:\n",
        "1. Encode the existing categories with a number corresponding to each category\n",
        "2. Make a dictionary storing the relationship between the original string category and the number we've replaced it with (categorical_names)\n",
        "3. Make a function that we can use down the line to transform categorical variables into dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eetKPh7hq9IW"
      },
      "outputs": [],
      "source": [
        "categorical_names = {}\n",
        "for feature in categorical_features:\n",
        "    print(\"Feature: \", feature)\n",
        "    # Use label encoder to map categories to numbers\n",
        "    le = sklearn.preprocessing.LabelEncoder()\n",
        "    le.fit(data.iloc[:, feature])\n",
        "    # Replace the categories with corresponding numbers in the original data\n",
        "    data.iloc[:, feature] = le.transform(data.iloc[:, feature])\n",
        "    # Store and print the mappings for reference later\n",
        "    categorical_names[feature] = le.classes_\n",
        "    print(categorical_names[feature])\n",
        "    print(\"==================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9AZ7Yhgq9IX"
      },
      "outputs": [],
      "source": [
        "# This variable is where we store the original names of each category for each variable\n",
        "categorical_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXlL0fvq9IZ"
      },
      "outputs": [],
      "source": [
        "# We can (and will) use this encoder function to transform the categorical columns into dummies--\n",
        "# but we can't do that to the original dataset if we want to use LIME\n",
        "encoder = ColumnTransformer(transformers=[('get_dummies', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
        "encoder = encoder.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUZI7wvOq9Ia"
      },
      "source": [
        "### Split into training and test sets\n",
        "\n",
        "We will us an 80/20 train-test split. We won't be doing any hyperparameter tuning during this lab, so no need to worry about a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dorpzcSgq9Ib"
      },
      "outputs": [],
      "source": [
        "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80, random_state=10)\n",
        "print(\"Train shape: \", train.shape)\n",
        "print(\"Test shape: \", test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfD3plyiq9Ib"
      },
      "source": [
        "***\n",
        "## Part 2: Train a model\n",
        "\n",
        "We will be using gradient boosted decision trees as implemented by the [xgboost](https://github.com/dmlc/xgboost) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNp3U31xq9Ic"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "gbtree = xgboost.XGBClassifier(n_estimators=200, max_depth=5)\n",
        "gbtree.fit(encoder.transform(train), labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGqRGKPWq9Id"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "pred_labels_test = gbtree.predict(encoder.transform(test))\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "print(\"Test set accuracy: \", sklearn.metrics.accuracy_score(labels_test, pred_labels_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQL9f5XGq9Ie"
      },
      "source": [
        "***\n",
        "## Part 3: Explaining predictions the usual way\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUV4Lpp9q9Ig"
      },
      "source": [
        "### **Questions:**\n",
        "*Hint:* Google is your best friend!\n",
        "1. How would you normally explain the predictions of a single decision tree?\n",
        "\n",
        "    **Write answer here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrTHcz9ASiOl"
      },
      "source": [
        "2. What is a gradient boosted decision tree?\n",
        "\n",
        "    **Write answer here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84wwbcOXSjx1"
      },
      "source": [
        "3. Does that change for a gradient boosted decision tree?\n",
        "\n",
        "    **Write answer here:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vvh_RxXR3Lg"
      },
      "source": [
        "We're going to be looking at a measure of feature importance. This is calculated for a single tree by counting how many splits occured on each variable. We arrive at a feature importance for the entire model by averaging the score for each feature across all trees in the forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crE6trorq9Ih"
      },
      "outputs": [],
      "source": [
        "# Get feature names for the transformed dataset so that they'll show up on the plot -- don't worry about this bit\n",
        "gbtree_features_orig = gbtree.get_booster().feature_names\n",
        "gbtree_features = []\n",
        "for cat_var in categorical_names:\n",
        "    cat_var_names = [feature_names[cat_var] +\" = \" + cat for cat in categorical_names[cat_var]]\n",
        "    gbtree_features.extend(cat_var_names)\n",
        "gbtree_features.extend([feature_names[i] for i in range(len(feature_names)) if i not in categorical_features])\n",
        "gbtree.get_booster().feature_names = gbtree_features\n",
        "\n",
        "# Plot feature importances\n",
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "plot_importance(gbtree.get_booster())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd-cRCaoq9Ii"
      },
      "source": [
        "Pick an interesting feature with a high importance in the plot above. Can you figure out the relationship of that feature with the outcome variable? (Is a higher or lower value more likely to indicate a high-income individual? For binary variables, is it 0 or 1?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6xqC8xaq9Ii"
      },
      "outputs": [],
      "source": [
        "# TODO - Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1NGR73q9I4"
      },
      "source": [
        "The plot_importance function takes a parameter called 'importance_type', which you can read more about [in the documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting). Try changing the importance type and re-making the plot. How does our explanation change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMZtvdzXq9I5"
      },
      "outputs": [],
      "source": [
        "# TODO - Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Na9ju_q9I6"
      },
      "outputs": [],
      "source": [
        "# Replace the original feature names, which LIME will expect\n",
        "gbtree.get_booster().feature_names = gbtree_features_orig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAhSESA_q9I6"
      },
      "source": [
        "***\n",
        "## Part 4: Explaining predictions with LIME\n",
        "\n",
        "Time for LIME!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hov3P0uPx-O"
      },
      "source": [
        "### LIME Tabular Explainer\n",
        "\n",
        "The tabular explainer is how we get locally interpretable explanations for classification problems.\n",
        "\n",
        "First, we initialize an explainer object that takes in all the information we stored/encoded earlier about the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6RMsMWsq9I7"
      },
      "outputs": [],
      "source": [
        "explainer = lime.lime_tabular.LimeTabularExplainer(train.values,\n",
        "                                                   feature_names=feature_names,\n",
        "                                                   class_names=class_names,\n",
        "                                                   categorical_features=categorical_features,\n",
        "                                                   categorical_names=categorical_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8ZuaqIq9I7"
      },
      "source": [
        "Next, we need to define a single function that takes in the form of data that LIME expects and returns the type of prediction that LIME expects. In particular,\n",
        "- the input should be a numpy array (which we can get from a pandas df using .values)\n",
        "- the input features should be human-understandable\n",
        "- the input data should have each categorical variable in a single column\n",
        "- the output should be a predicted probability (not a predicted class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JygO8Hu7q9I8"
      },
      "outputs": [],
      "source": [
        "predict_fn = lambda x: gbtree.predict_proba(encoder.transform(x)).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEwAQi8Zq9I9"
      },
      "source": [
        "Now we have everything we need to use the explainer. Let's get an explanation for one of the examples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tn42gAtq9I-"
      },
      "outputs": [],
      "source": [
        "i = 2\n",
        "print('Actual class: ', labels_test[i])\n",
        "# Get explanation\n",
        "exp = explainer.explain_instance((test.values[i]), predict_fn, num_features=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Gq5gfTq9I_"
      },
      "outputs": [],
      "source": [
        "# Visualize the explanation\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU6Z7Vchq9JA"
      },
      "source": [
        "The explanations can also be exported as an html page (which we can render here in this notebook), using D3.js to render graphs. You could also save the html page to a file if you wanted to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC2AmRIiq9JA"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-e0HCinq9JJ"
      },
      "source": [
        "The explanation can also be presented as a list of weighted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDedXxbZq9JK"
      },
      "outputs": [],
      "source": [
        "exp.as_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEwIFNwiq9JK"
      },
      "source": [
        "### **Questions:**\n",
        "4. How do we interpret this plot? That is, what do red and green mean? What's on the x axis?\n",
        "\n",
        "    **Write answer here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmidheqATuDq"
      },
      "source": [
        "5. How does this explanation differ from the feature importance explanation given above?\n",
        "\n",
        "    **Write answer here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkNUXfPNUHu5"
      },
      "source": [
        "### **TODO - Generate LIME explanations (in whichever format you chose) for some other examples in the test set. Are similar features important? Add text + code cells below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncnzPDQUq9JL"
      },
      "source": [
        "### LIME Submodular Picker\n",
        "\n",
        "As we just saw, explanations can vary a lot depending on what instance we pick. While this is great for explaining a single prediction, it makes it hard to give someone general intuition for \"how the model makes decisions.\" That's where the submodular picker comes in. It picks useful, representative examples that together give global explanation for the model.\n",
        "\n",
        "In broad strokes, the algorithm does the following:\n",
        "1. Calculate an explanation for all examples in the dataset\n",
        "2. Determine which features are important in explaining a lot of predictions -- that is, features that seem globally important\n",
        "3. Select (greedily) examples where the top globally important feature is part of the local explanation for that one example's prediction\n",
        "3. Continue selecting examples until we've covered as many of the globally important features as possible, constrained by the number of features that the user wants returned (num_exps_desired)\n",
        "\n",
        "You can read the details of how this is done in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMga3Fkzq9JL"
      },
      "outputs": [],
      "source": [
        "# Initialize the SP object\n",
        "sp_obj = submodular_pick.SubmodularPick(explainer, train.values, predict_fn, sample_size=10,\n",
        "                                        num_features=5, num_exps_desired=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ub4Zpf0q9JM"
      },
      "source": [
        "The attribute V tells us the best indices from the test set to explain the overall predictions of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFWAZSh6q9JM"
      },
      "outputs": [],
      "source": [
        "sp_obj.V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_dwJfBGq9JN"
      },
      "source": [
        "Now, we can get explantions for each of those examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9VwYBLwq9JN"
      },
      "outputs": [],
      "source": [
        "for ind in sp_obj.V:\n",
        "    exp = explainer.explain_instance(test.values[ind], predict_fn, num_features=5)\n",
        "    print(\"Actual class: \", labels_test[ind])\n",
        "    exp.show_in_notebook(show_all=False)\n",
        "    print(\"==========================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ktCjp5zq9Jr"
      },
      "source": [
        "### **Question:**\n",
        "\n",
        "6. Based on these carefully chosen examples, what would you say to someone who wanted to know how our model makes decisions?\n",
        "\n",
        "    **Write answer here:**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
