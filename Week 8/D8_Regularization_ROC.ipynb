{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xG6krskFAN-"
      },
      "source": [
        "***\n",
        "**Author:** Peter Lu\n",
        "\n",
        "Data Science Ethics (UCR - Winter 2024)\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUcNrWXAFNRB"
      },
      "source": [
        "# Regularization and ROC Curves\n",
        "This notebook will explore a method for helping predictive models better generalize to unseen data. Then, we'll see some absolute metrics that build off of FPR and TNR. They provide interpretable measures for understanding how well a model is understanding the classes in our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xJYV9KGBmx"
      },
      "source": [
        "***\n",
        "## Regularization\n",
        "**Regularization** is a technique that attempts to accomplish two goals:\n",
        "1.  Improve a model's generalizability on unseen data\n",
        "2.  Keep a model from learning the noise in our data rather than the patterns in the data\n",
        "\n",
        "How does regularization accomplish this? It penalizes large parameters that overly focus on certain features. Consider a scenario where you learn the following linear regression model to predict continuous labels:\n",
        "$$y = 1000x_1 + 50x_2 + x_3 + 5$$\n",
        "where\n",
        "*  $y$ = House Price\n",
        "*  $x_1$ = Square Footage\n",
        "*  $x_2$ = # of rooms\n",
        "*  $x_3$ = # of shopping centers in a 5 mile radius\n",
        "\n",
        "Based on the data the model was fit, it learned that Square Footage plays the greatest role, by far, in predicting house price. We may know that other features are more important, but the training data the model learned from didn't represent this fact. Regularization trains a model then \"punishes\" large coefficients to give smaller coefficients more power in the prediction process. With regularization, we may get a model like this:\n",
        "$$y_{\\text{reg}} = 600x_1 + 500x_2 + 80x_3 + 15.$$\n",
        "\n",
        "This model will generalize better to unseen data because there's less of a reliance on square footage, a feature that may not be as important in pricing houses in some areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOCnvFJhJcTO"
      },
      "source": [
        "Let's see how regularization improves model performance with the MNIST dataset! In this setting, we will train a logistic regression model to classify if an image is a handwritten image of a 5 or not. We'll work with a low number of training samples, with respect to the number of features, to simulate a setting where we have few samples to learn from and many samples to classify. Will regularization help us generalize?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YxShQKLwiE9O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets.mnist import load_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eObDwSkrk_aC"
      },
      "outputs": [],
      "source": [
        "# Load data + check dimensions\n",
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "X_train, y_train = X_train[:500], y_train[:500]\n",
        "print(f'Image Resolution: {X_train.shape[1:]}')\n",
        "print(f'Number of Features: {X_train.shape[1] * X_train.shape[2]}')\n",
        "print(f'Training Samples: {X_train.shape[0]}')\n",
        "print(f'Testing Samples: {X_test.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGWB9OrlLdSW"
      },
      "outputs": [],
      "source": [
        "# One-hot encode images as \"1\" for \"image of 5\" and \"0\" for \"not image of 5\"\n",
        "y_train_5 = np.where(y_train == 5, 1, 0)\n",
        "y_test_5 = np.where(y_test == 5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-fu8sIfLwSh"
      },
      "outputs": [],
      "source": [
        "# Visualize images of 5\n",
        "plt.figure(figsize = (8, 6))\n",
        "for i in range(4):\n",
        "  plt.subplot(2, 2, i + 1)\n",
        "  plt.imshow(X_train[y_train_5 == 1][i], cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.suptitle('Training Samples: 5')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19OYvre-MEnC"
      },
      "outputs": [],
      "source": [
        "# Visualize images of numbers that aren't 5\n",
        "plt.figure(figsize = (8, 6))\n",
        "for i in range(4):\n",
        "  plt.subplot(2, 2, i + 1)\n",
        "  plt.imshow(X_train[y_train_5 == 0][i], cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.suptitle('Training Samples: Not 5')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHtJVFCuMeNW"
      },
      "source": [
        "Logistic regression cannot take image matrices as input. They must be flattened, or every row of the image matrix must be concatenated into a long vector! For reference, logistic regression takes the following form:\n",
        "$$p(y = 1|x) = \\frac{1}{1 + e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}}$$\n",
        "$$p(y = 0|x) = 1 - p(y = 1|x) = \\frac{e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}}{1 + e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\boldsymbol{\\theta}^T\\boldsymbol{x} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n.$$\n",
        "\n",
        "Notice this is similar to linear regression, but the model is plugged into the funcion above: the **sigmoid function**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB4tXi1fmXAY"
      },
      "outputs": [],
      "source": [
        "# Flatten the matrices\n",
        "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
        "X_test_flattened = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "X_train_flattened[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-gCMcWQO0xN"
      },
      "outputs": [],
      "source": [
        "X_train_flattened[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpHTwBSePIgV"
      },
      "source": [
        "Scaling or **standardizing** is a common method used to preprocess data for training. It helps models not only learn better because data will be on a similar scale, but they also learn faster. It performs the following transformation across each feature:\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzJ0fPAEOo6y"
      },
      "outputs": [],
      "source": [
        "# Standardize features for better training\n",
        "scaler = StandardScaler()\n",
        "X_train_flattened = scaler.fit_transform(X_train_flattened)\n",
        "X_test_flattened = scaler.transform(X_test_flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEByZMHAP3u6"
      },
      "source": [
        "Now, let's compare a regularized model vs. a non-regularized model! There are many types of regularizers, but we'll use L2 regularization with a $C = 0.1$. The lower $C$ is, the stronger the penalization of large coefficients is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8eMt7gEoaSK"
      },
      "outputs": [],
      "source": [
        "model_no_reg = LogisticRegression(penalty = None)\n",
        "model_no_reg.fit(X_train_flattened, y_train_5)\n",
        "print(f'Training Accuracy: {model_no_reg.score(X_train_flattened, y_train_5):.2%}')\n",
        "print(f'Test Accuracy: {model_no_reg.score(X_test_flattened, y_test_5):.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYQ4Aow0rPZQ"
      },
      "outputs": [],
      "source": [
        "model_reg = LogisticRegression(penalty = 'l2', C = 0.1)\n",
        "model_reg.fit(X_train_flattened, y_train_5)\n",
        "print(f'Training Accuracy: {model_reg.score(X_train_flattened, y_train_5):.2%}')\n",
        "print(f'Test Accuracy: {model_reg.score(X_test_flattened, y_test_5):.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMcvm2XISiSN"
      },
      "source": [
        "***\n",
        "### ROC Curves\n",
        "The **receiver operating characteristic (ROC) curve** plots the *true positive rate (TPR)* vs. the *false positive rate (FPR)* against each other at varying thresholds.\n",
        "\n",
        "Given a model, we use its predicted probabilities that a sample is a given class (0/1) and provide a threshold for how to determine if a sample falls into the class 0 or class 1. For example, if a model predicts sample $x$ to have a predicted probability of 0.6 of being in class 1, a threshold value of 0.8 would now label $x$ as being in class 0 because it did not exceed 0.8.\n",
        "\n",
        "We do this procedure for several different threshold values from 0 to 1. This gives us different TPRs and FPRs that we can plot. Let's see this procedure below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwxvoFdmQXop"
      },
      "outputs": [],
      "source": [
        "# Compute probabilities of being a 5\n",
        "y_proba_no_reg = model_no_reg.predict_proba(X_test_flattened)[:, 1]\n",
        "y_proba_reg = model_reg.predict_proba(X_test_flattened)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la77MhFEUK-J"
      },
      "outputs": [],
      "source": [
        "# Plot non-5\n",
        "plt.imshow(X_test[0], cmap = 'gray')\n",
        "plt.title(f'Probability of 5 with no reg: {y_proba_no_reg[0]:.2}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR8qht0uUVL4"
      },
      "outputs": [],
      "source": [
        "# Plot 5\n",
        "img_5 = y_test_5 == 1\n",
        "plt.imshow(X_test[img_5][10], cmap = 'gray')\n",
        "plt.title(f'Probability of 5 with no reg: {y_proba_no_reg[img_5][10]:.2}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEzqMwBWv6P9"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curves for regularized and non-regularized models\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr_no_reg, tpr_no_reg, thresholds_no_reg = roc_curve(y_test_5, y_proba_no_reg)\n",
        "fpr_reg, tpr_reg, thresholds_reg = roc_curve(y_test_5, y_proba_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU18TGuAXGRG"
      },
      "outputs": [],
      "source": [
        "# Example of how ROC computes an FPR using a threshold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "e = thresholds_no_reg[500]\n",
        "y_new = np.where(y_proba_no_reg > e, 1, 0)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_5, y_new).ravel()\n",
        "print(f'Computed FPR: {fp / (fp + tn)}', end = '\\n\\n')\n",
        "\n",
        "print(f'Threshold: {e}')\n",
        "print(f'Threshold FPR: {fpr_no_reg[500]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hywSaFYmZip0"
      },
      "source": [
        "***\n",
        "### Area Under the Curve (AUC)\n",
        "After computing the FPR and TPR for several different thresholds, we then compute the **area under the curve (AUC)** which is a measure of a model's performance in distinguishing between two classes. Generally,\n",
        "*  AUC = 1 means a model perfectly classifies the two classes\n",
        "*  AUC = 0.5 means a model classifies equivalently to a coin flip\n",
        "\n",
        "Lower the AUC, the worse the model is at distinguishing the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlLhsvuJy2dy"
      },
      "outputs": [],
      "source": [
        "auc_no_reg = auc(fpr_no_reg, tpr_no_reg)\n",
        "auc_reg = auc(fpr_reg, tpr_reg)\n",
        "\n",
        "plt.plot(fpr_no_reg, tpr_no_reg, color = 'red', label = f'No Reg AUC: {auc_no_reg:.3f}')\n",
        "plt.plot(fpr_reg, tpr_reg, color = 'green', label = f'Reg AUC: {auc_reg:.3f}')\n",
        "plt.title('ROC Curves: Reg vs. Non-Reg')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3uhaQoBcKyc"
      },
      "outputs": [],
      "source": [
        "Cs = np.linspace(0.00001, 1, 10)\n",
        "for C in Cs:\n",
        "  model = LogisticRegression(penalty = 'l2', C = C)\n",
        "  model.fit(X_train_flattened, y_train_5)\n",
        "  y_proba = model.predict_proba(X_test_flattened)[:, 1]\n",
        "  fpr, tpr, _ = roc_curve(y_test_5, y_proba)\n",
        "  area = auc(fpr, tpr)\n",
        "  plt.plot(fpr, tpr, label = f'AUC: {area:.3f}, C = {C:.2f}')\n",
        "\n",
        "plt.title('ROC Curves with varying C\\'s')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sgmdcKlb1Z5"
      },
      "source": [
        "***\n",
        "Now that we've seen how to determine a model's capability at distinguishing between two classes, what can ROC curves and AUC tell us about bias and fairness? Let's load the COMPAS dataset and see if the bias/unfairness we detected before is a result of our model or a systemic issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdLFoYo6zH1r"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data as in assignment 1\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "df_compas = pd.read_csv(url)\n",
        "\n",
        "# Grab useful columns\n",
        "cols_to_keep = [\"id\", \"age\", \"c_charge_degree\", \"race\", \"age_cat\", \"score_text\",\n",
        "                \"sex\", \"priors_count\", \"days_b_screening_arrest\",\n",
        "                \"decile_score\", \"is_recid\", \"two_year_recid\"]\n",
        "\n",
        "df_selected = df_compas[cols_to_keep].copy()\n",
        "\n",
        "\n",
        "# Grab samples of interest\n",
        "df_analysis = df_selected[\n",
        "    (df_selected.score_text != \"N/A\") &\n",
        "    (df_selected.days_b_screening_arrest <= 30) &\n",
        "    (df_selected.days_b_screening_arrest >= -30) &\n",
        "    (df_selected.is_recid != -1) &\n",
        "    (df_selected.c_charge_degree != \"O\")\n",
        "    ].copy()\n",
        "\n",
        "df_analysis[\"decile_score\"] = pd.to_numeric(df_analysis[\"decile_score\"])\n",
        "df_logistic = df_analysis.copy()\n",
        "\n",
        "# one-hot encoding\n",
        "df_logistic = pd.get_dummies(df_logistic,\n",
        "                             columns = [\"c_charge_degree\", \"race\",\n",
        "                                        \"age_cat\", \"sex\"])\n",
        "\n",
        "# Replace 'Low' with 0 and `Medium` / `High` with 1\n",
        "df_logistic[\"label\"] = np.where(df_logistic[\"score_text\"] != \"Low\", 1, 0)\n",
        "df_logistic.drop(columns = ['score_text'], inplace = True)\n",
        "\n",
        "# Format column names\n",
        "df_logistic.columns = df_logistic.columns.str.replace(' ', '_')\n",
        "df_logistic.columns = df_logistic.columns.str.replace('-', '_')\n",
        "\n",
        "renamed_cols = {'age_cat_25___45':'age_cat_25_to_45',\n",
        "                'c_charge_degree_F':'Felony',\n",
        "                'c_charge_degree_M':'Misdemeanor'}\n",
        "\n",
        "df_logistic = df_logistic.rename(columns = renamed_cols)\n",
        "X = df_logistic[['priors_count', 'two_year_recid', 'Misdemeanor', 'age_cat_Greater_than_45', 'age_cat_Less_than_25', 'race_African_American', 'race_Asian', 'race_Hispanic', 'race_Native_American', 'race_Other', 'sex_Female']]\n",
        "y = df_logistic['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.95, random_state = 5)\n",
        "print(f'Training samples: {X_train.shape[0]}')\n",
        "print(f'Test samples: {X_test.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRfh-e8Y7U6B"
      },
      "outputs": [],
      "source": [
        "# Train the models and check their accuracies\n",
        "model_no_reg = LogisticRegression(penalty = None)\n",
        "model_no_reg.fit(X_train, y_train)\n",
        "\n",
        "print(f'Training Accuracy: {model_no_reg.score(X_train, y_train):.2%}')\n",
        "print(f'Test Accuracy: {model_no_reg.score(X_test, y_test):.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kklEXn8ifv49"
      },
      "outputs": [],
      "source": [
        "model_reg = LogisticRegression(penalty = 'l2', C = 0.1)\n",
        "model_reg.fit(X_train, y_train)\n",
        "\n",
        "print(f'Training Accuracy: {model_reg.score(X_train, y_train):.2%}')\n",
        "print(f'Test Accuracy: {model_reg.score(X_test, y_test):.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR2kMiCv8iA1"
      },
      "outputs": [],
      "source": [
        "# Generate ROC curves and calculate AUC\n",
        "y_proba_no_reg = model_no_reg.predict_proba(X_test)[:, 1]\n",
        "y_proba_reg = model_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "fpr_no_reg, tpr_no_reg, thresholds_no_reg = roc_curve(y_test, y_proba_no_reg)\n",
        "auc_no_reg = auc(fpr_no_reg, tpr_no_reg)\n",
        "\n",
        "fpr_reg, tpr_reg, thresholds_reg = roc_curve(y_test, y_proba_reg)\n",
        "auc_reg = auc(fpr_reg, tpr_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05BCiHX08ql9"
      },
      "outputs": [],
      "source": [
        "plt.plot(fpr_no_reg, tpr_no_reg, color = 'red', label = f'No Reg AUC: {auc_no_reg:.3f}')\n",
        "plt.plot(fpr_reg, tpr_reg, color = 'green', label = f'Reg AUC: {auc_reg:.3f}')\n",
        "plt.title('ROC Curves: Reg vs. Non-Reg')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF39CXDcgV-d"
      },
      "source": [
        "From the above plot, this tells us that even after regularizing, the bias/unfairness we saw before is *most likely* not a result of our model. We can make this conclusion because regularization penalizes features that may overly influence a model's decision. After regularization, our predictive power remained about the same, meaning that there's no feature or set of features overly influencing our predictions. In other words, the model isn't biased toward race, sex, or age group. This *may suggest* the bias/unfairness is **systemic**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaY57Shhg_hr"
      },
      "source": [
        "Let's check this hypothesis specifically on the models' performance on samples from the African American population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjbFMXFv9gZd"
      },
      "outputs": [],
      "source": [
        "# Grab samples from African American community\n",
        "test_idxs = np.where(X_test['race_African_American'] == 1)\n",
        "aa_proba_no_reg = model_no_reg.predict_proba(X_test.iloc[test_idxs])[:, 1]\n",
        "aa_proba_reg = model_reg.predict_proba(X_test.iloc[test_idxs])[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bySX9ulL-qnK"
      },
      "outputs": [],
      "source": [
        "fpr_no_reg, tpr_no_reg, thresholds_no_reg = roc_curve(y_test.iloc[test_idxs], aa_proba_no_reg)\n",
        "auc_no_reg = auc(fpr_no_reg, tpr_no_reg)\n",
        "\n",
        "fpr_reg, tpr_reg, thresholds_reg = roc_curve(y_test.iloc[test_idxs], aa_proba_reg)\n",
        "auc_reg = auc(fpr_reg, tpr_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5va-nFAu-1Ok"
      },
      "outputs": [],
      "source": [
        "plt.plot(fpr_no_reg, tpr_no_reg, color = 'red', label = f'No Reg AUC: {auc_no_reg:.3f}')\n",
        "plt.plot(fpr_reg, tpr_reg, color = 'green', label = f'Reg AUC: {auc_reg:.3f}')\n",
        "plt.title('ROC Curves: Reg vs. Non-Reg')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSfm_SYY_B7f"
      },
      "outputs": [],
      "source": [
        "Cs = np.linspace(0.00001, 1, 10)\n",
        "X_test_aa = X_test.iloc[test_idxs]\n",
        "y_test_aa = y_test.iloc[test_idxs]\n",
        "\n",
        "for i, C in enumerate(Cs):\n",
        "  if i == 0:\n",
        "    continue\n",
        "  model = LogisticRegression(penalty = 'l2', C = C)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_proba = model.predict_proba(X_test_aa)[:, 1]\n",
        "  fpr, tpr, _ = roc_curve(y_test_aa, y_proba)\n",
        "  area = auc(fpr, tpr)\n",
        "  plt.plot(fpr, tpr, label = f'AUC: {area:.3f}, C = {C:.2f}')\n",
        "\n",
        "plt.title('ROC Curves with varying C\\'s')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtkV3hwRvJrr"
      },
      "source": [
        "### What now?\n",
        "To further investigate:\n",
        "*  Investigate how the data was collected\n",
        "*  Try different models with different levels of regularization\n",
        "*  Test more complex models that may capture the bias/unfairness we saw previously"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
