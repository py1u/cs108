{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "\n",
    "# Prints out a set nicely\n",
    "# names is an optional list of the names for each of the (integer) items\n",
    "def settostr(s, names=None):\n",
    "    if names is None:\n",
    "        elems = [str(e) for e in s]\n",
    "    else:\n",
    "        elems = [names[e] for e in s]\n",
    "    return \"{\" + (\", \".join(elems)) + \"}\"\n",
    "\n",
    "# Loads data from a file\n",
    "def loaddata(filename):\n",
    "    with open(filename) as f:\n",
    "        nitems = int(f.readline())\n",
    "        names = [f.readline().strip() for i in range(nitems)]\n",
    "        nrows = int(f.readline())\n",
    "        data = [[int(s) for s in f.readline().split()] for i in range(nrows)]\n",
    "        f.close()\n",
    "        return names, data, nitems\n",
    "\n",
    "def learnrules(numitems, data, minsupport, minconfidence):\n",
    "    # Generate frequent itemsets using the Apriori algorithm\n",
    "    def apriori(data, minsupport):\n",
    "        itemsets = [frozenset([item]) for item in range(numitems)]\n",
    "        frequent_itemsets = []\n",
    "        while itemsets:\n",
    "            # Generate candidate itemsets\n",
    "            candidates = set()\n",
    "            for itemset in itemsets:\n",
    "                for item in range(itemset[-1] + 1, numitems):\n",
    "                    candidates.add(itemset | frozenset([item]))\n",
    "            # Count support for each candidate itemset\n",
    "            support_counts = {}\n",
    "            for transaction in data:\n",
    "                for candidate in candidates:\n",
    "                    if candidate.issubset(transaction):\n",
    "                        support_counts[candidate] = support_counts.get(candidate, 0) + 1\n",
    "            # Prune candidates based on minimum support\n",
    "            itemsets = []\n",
    "            for candidate, support in support_counts.items():\n",
    "                if support >= minsupport:\n",
    "                    itemsets.append(candidate)\n",
    "                    frequent_itemsets.append((candidate, support))\n",
    "            # Join itemsets for the next iteration\n",
    "            itemsets = list(chain(*[combinations(itemset, r + 2) for itemset in itemsets]))\n",
    "        return frequent_itemsets\n",
    "\n",
    "    # Generate association rules from frequent itemsets\n",
    "    def generate_rules(frequent_itemsets, minconfidence):\n",
    "        rules = []\n",
    "        for itemset, support in frequent_itemsets:\n",
    "            if len(itemset) >= 2:\n",
    "                for i in range(1, len(itemset)):\n",
    "                    for antecedent in combinations(itemset, i):\n",
    "                        antecedent = frozenset(antecedent)\n",
    "                        consequent = itemset - antecedent\n",
    "                        confidence = support / frequent_itemsets_dict[antecedent]\n",
    "                        if confidence >= minconfidence:\n",
    "                            rules.append((antecedent, consequent, support, confidence))\n",
    "        return sorted(rules, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    frequent_itemsets = apriori(data, minsupport)\n",
    "    frequent_itemsets_dict = {itemset: support for itemset, support in frequent_itemsets}\n",
    "    rules = generate_rules(frequent_itemsets, minconfidence)\n",
    "    return rules\n",
    "\n",
    "def writerules(rules, data, itemnames):\n",
    "    for antecedent, consequent, support, confidence in rules:\n",
    "        antecedent_str = settostr(antecedent, itemnames)\n",
    "        consequent_str = settostr(consequent, itemnames)\n",
    "        support_str = \"{:7.4f}\".format(support / len(data))\n",
    "        confidence_str = \"{:7.4f}\".format(confidence)\n",
    "        print(f\"{support_str} {confidence_str} {antecedent_str} => {consequent_str}\")\n",
    "\n",
    "def printruleset(datasetfilename, minsupport, minconfidence):\n",
    "    itemnames, data, numitems = loaddata(datasetfilename)\n",
    "    rules = learnrules(numitems, data, minsupport, minconfidence)\n",
    "    writerules(rules, data, itemnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pseudocode idea\n",
    "\n",
    "function Apriori-Gen(Li−1)\n",
    "▷ From frequent items sets of size i − 1, generate candidate frequent items sets of size i\n",
    "Ci ← {}\n",
    "for all I ∈ Li−1 do\n",
    "for all J ∈ Li−1, J ̸ = I do\n",
    "if |I S J| = i then\n",
    "Ci ← Ci\n",
    "S{I S J}\n",
    "return Ci\n",
    "\n",
    "\n",
    "function Apriori(I , D, smin)\n",
    "▷ From items (I ), data (D), and smin, generate all frequent itemsets\n",
    "L1 ← {{i} | i ∈ I ∧ s({i}) ≥ smin}\n",
    "L ← L1\n",
    "i ← 1\n",
    "while |Li | > 0 do\n",
    "i ← i + 1\n",
    "Ci ← Apriori-Gen(Li−1) ▷ Ci are the blue and orange nodes on level i\n",
    "Li ← {} ▷ Li (will be) the blue nodes on level i\n",
    "for all c ∈ Ci do\n",
    "if s(c) ≥ smin then\n",
    "Li ← Li\n",
    "S{c}\n",
    "L ← L S Li\n",
    "return L\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
